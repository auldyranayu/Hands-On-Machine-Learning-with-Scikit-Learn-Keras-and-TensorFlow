{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: Training Models\n",
        "\n",
        "Pada chapter ini, kita membahas bagaimana **model Machine Learning dilatih** menggunakan data.\n",
        "Fokus utama chapter ini adalah **model regresi**, teknik optimisasi, serta konsep penting seperti\n",
        "underfitting, overfitting, dan regularisasi.\n",
        "\n",
        "Chapter ini merupakan fondasi penting sebelum masuk ke model yang lebih kompleks seperti\n",
        "Support Vector Machines dan Neural Networks.\n"
      ],
      "metadata": {
        "id": "tQFNOGUhHZTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Linear Regression\n",
        "\n",
        "Linear Regression adalah salah satu algoritma Machine Learning paling sederhana dan paling sering digunakan.\n",
        "Model ini mencoba mempelajari hubungan linear antara fitur input dan target output.\n",
        "\n",
        "Bentuk umum model Linear Regression:\n",
        "\n",
        "\\[\n",
        "\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
        "\\]\n",
        "\n",
        "Tujuan pelatihan model adalah menemukan parameter θ (theta) yang meminimalkan kesalahan prediksi.\n"
      ],
      "metadata": {
        "id": "CTDlt7Y6Hdwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Model Linear Regression\n",
        "\n",
        "Model Linear Regression mengasumsikan bahwa hubungan antara variabel bersifat linear.\n",
        "Kesalahan prediksi diukur menggunakan **Mean Squared Error (MSE)** sebagai fungsi biaya.\n"
      ],
      "metadata": {
        "id": "z9xLVeQzIEbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generating Linear Data\n",
        "\n",
        "Sebelum melatih model, kita akan membuat dataset sederhana yang memiliki hubungan linear.\n",
        "Dataset ini akan digunakan untuk memahami cara kerja algoritma regresi secara intuitif.\n"
      ],
      "metadata": {
        "id": "5hJX2_kFIHOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUP7aDqYGyK0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training Linear Regression Using the Normal Equation\n",
        "\n",
        "Salah satu cara menghitung parameter Linear Regression adalah menggunakan **Normal Equation**.\n",
        "Pendekatan ini memberikan solusi analitik langsung tanpa iterasi.\n"
      ],
      "metadata": {
        "id": "NLP_9oC-IMRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Normal Equation\n",
        "\n",
        "Normal Equation dirumuskan sebagai:\n",
        "\n",
        "\\[\n",
        "\\theta = (X^T X)^{-1} X^T y\n",
        "\\]\n",
        "\n",
        "Pendekatan ini bekerja baik untuk dataset kecil hingga menengah, tetapi tidak efisien\n",
        "untuk dataset yang sangat besar.\n"
      ],
      "metadata": {
        "id": "AMhH9noDIQUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "theta_best\n"
      ],
      "metadata": {
        "id": "dDFp8s7aIRr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Making Predictions\n",
        "\n",
        "Setelah parameter model diperoleh, kita dapat menggunakan model tersebut untuk\n",
        "melakukan prediksi pada data baru.\n"
      ],
      "metadata": {
        "id": "wNVYChI9ITZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        "\n",
        "y_predict = X_new_b.dot(theta_best)\n",
        "y_predict\n"
      ],
      "metadata": {
        "id": "ZqQzxGCfIU2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Linear Regression Using Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi Linear Regression yang efisien dan mudah digunakan.\n",
        "Pendekatan ini lebih praktis dibandingkan implementasi manual.\n"
      ],
      "metadata": {
        "id": "hbAfUmsIIVj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "\n",
        "lin_reg.intercept_, lin_reg.coef_\n"
      ],
      "metadata": {
        "id": "eC5qB3ELIXGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Gradient Descent\n",
        "\n",
        "Gradient Descent adalah metode optimisasi iteratif yang digunakan untuk meminimalkan fungsi biaya.\n",
        "Metode ini sangat penting ketika Normal Equation tidak praktis digunakan.\n"
      ],
      "metadata": {
        "id": "l6-mu2xCIY1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 The Cost Function\n",
        "\n",
        "Pada Linear Regression, fungsi biaya yang digunakan adalah **Mean Squared Error (MSE)**.\n",
        "Gradient Descent bekerja dengan menuruni gradien fungsi ini.\n"
      ],
      "metadata": {
        "id": "fUhHT7uqIbCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Batch Gradient Descent\n",
        "\n",
        "Batch Gradient Descent menggunakan seluruh dataset pada setiap langkah pembaruan parameter.\n",
        "Pendekatan ini stabil tetapi bisa sangat lambat untuk dataset besar.\n"
      ],
      "metadata": {
        "id": "k26senyeIcUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.1\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "theta = np.random.randn(2, 1)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "theta\n"
      ],
      "metadata": {
        "id": "RRMxsFwBIfbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Learning Rate\n",
        "\n",
        "**Learning rate** (sering dilambangkan dengan simbol η atau α) merupakan salah satu hiperparameter paling penting dalam algoritma **Gradient Descent**. Parameter ini mengontrol **besar langkah perubahan parameter model** (bobot dan bias) pada setiap iterasi selama proses pelatihan.\n",
        "\n",
        "Secara intuitif, learning rate menentukan seberapa cepat model bergerak menuju nilai minimum dari fungsi biaya (*cost function*). Pemilihan nilai learning rate yang tepat sangat berpengaruh terhadap keberhasilan proses optimasi.\n",
        "\n",
        "Beberapa kondisi yang umum terjadi terkait pemilihan learning rate adalah sebagai berikut:\n",
        "\n",
        "- **Learning rate terlalu kecil**  \n",
        "  Jika nilai learning rate sangat kecil, setiap pembaruan parameter akan dilakukan dengan langkah yang sangat pendek. Akibatnya, proses konvergensi menuju minimum fungsi biaya menjadi sangat lambat. Model membutuhkan banyak iterasi untuk mencapai solusi yang optimal, sehingga waktu pelatihan menjadi tidak efisien.\n",
        "\n",
        "- **Learning rate terlalu besar**  \n",
        "  Sebaliknya, jika learning rate terlalu besar, pembaruan parameter dapat melampaui titik minimum fungsi biaya. Kondisi ini menyebabkan algoritma gagal konvergen dan bahkan dapat mengalami **divergensi**, di mana nilai fungsi biaya justru semakin besar dari waktu ke waktu.\n",
        "\n",
        "Oleh karena itu, learning rate harus dipilih dengan hati-hati agar algoritma dapat mencapai konvergensi secara stabil dan efisien. Dalam praktik, nilai learning rate sering ditentukan melalui eksperimen atau menggunakan teknik penyesuaian otomatis seperti **learning schedule**.\n",
        "\n",
        "Dalam buku *Hands-On Machine Learning*, perilaku Gradient Descent dengan berbagai nilai learning rate biasanya divisualisasikan dalam bentuk grafik. Visualisasi tersebut menunjukkan perbedaan antara:\n",
        "- konvergensi yang stabil menuju minimum global,\n",
        "- osilasi di sekitar minimum,\n",
        "- serta kegagalan konvergensi akibat langkah pembaruan yang terlalu besar.\n",
        "\n",
        "Visualisasi ini membantu memberikan pemahaman intuitif mengenai dampak learning rate terhadap proses pelatihan model.\n"
      ],
      "metadata": {
        "id": "IkhANTsiLuaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "SGD memperbarui parameter menggunakan satu instance data pada setiap iterasi.\n",
        "Metode ini jauh lebih cepat tetapi lebih berisik dibandingkan Batch Gradient Descent.\n"
      ],
      "metadata": {
        "id": "aqAs-pvvIhVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Karakteristik Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Stochastic Gradient Descent (SGD) merupakan varian dari Gradient Descent yang melakukan pembaruan parameter menggunakan **satu sampel data secara acak** pada setiap iterasi. Pendekatan ini memberikan sejumlah karakteristik khas yang membedakannya dari Batch Gradient Descent.\n",
        "\n",
        "Beberapa karakteristik utama dari SGD antara lain:\n",
        "- Proses pembaruan parameter berlangsung sangat cepat karena hanya menggunakan satu instance data\n",
        "- Sangat efisien untuk dataset berskala besar dan cocok untuk skenario online learning\n",
        "- Jalur optimasi menuju minimum fungsi biaya cenderung berosilasi dan tidak stabil\n",
        "- Sulit mencapai minimum absolut secara presisi, namun mampu mendekati minimum global\n"
      ],
      "metadata": {
        "id": "g2T6m21KMOP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Learning Schedule\n",
        "\n",
        "Untuk mengurangi efek osilasi yang berlebihan pada Stochastic Gradient Descent, sering diterapkan strategi yang disebut **learning schedule**. Learning schedule adalah metode untuk **menurunkan nilai learning rate secara bertahap** seiring berjalannya proses training.\n",
        "\n",
        "Dengan learning rate yang semakin kecil:\n",
        "- Langkah pembaruan parameter menjadi lebih halus\n",
        "- Proses optimasi menjadi lebih stabil saat mendekati solusi optimal\n",
        "- Model memiliki peluang lebih besar untuk konvergen dengan baik\n"
      ],
      "metadata": {
        "id": "7Wwso0v1MRmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2, 1)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "\n",
        "theta"
      ],
      "metadata": {
        "id": "qslO6dkLMXgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Mini-batch Gradient Descent\n",
        "\n",
        "Mini-batch Gradient Descent merupakan kompromi antara Batch dan SGD.\n",
        "Metode ini menggunakan sebagian kecil data pada setiap iterasi.\n"
      ],
      "metadata": {
        "id": "fv9mJSLOIitl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Keunggulan Mini-batch Gradient Descent\n",
        "\n",
        "Mini-batch Gradient Descent merupakan kompromi antara Batch Gradient Descent dan Stochastic Gradient Descent. Metode ini melakukan pembaruan parameter dengan menggunakan sebagian kecil data (mini-batch) pada setiap iterasi.\n",
        "\n",
        "Beberapa keunggulan utama dari pendekatan ini antara lain:\n",
        "\n",
        "- Proses optimisasi lebih stabil dibandingkan Stochastic Gradient Descent karena gradien dihitung dari lebih dari satu sampel\n",
        "- Waktu komputasi lebih efisien dibandingkan Batch Gradient Descent karena tidak harus memproses seluruh dataset sekaligus\n",
        "- Sangat cocok digunakan pada perangkat keras modern seperti CPU multi-core dan GPU karena mendukung komputasi paralel\n",
        "\n",
        "Berkat keseimbangan antara kecepatan dan stabilitas tersebut, Mini-batch Gradient Descent menjadi metode optimisasi yang paling umum digunakan dalam pelatihan model Machine Learning dan Deep Learning.\n",
        "\n",
        "Dalam literatur, perbedaan perilaku Batch, Stochastic, dan Mini-batch Gradient Descent sering divisualisasikan untuk menunjukkan variasi jalur konvergensi menuju minimum fungsi biaya, sebagaimana diilustrasikan pada buku referensi.\n"
      ],
      "metadata": {
        "id": "TaUcT2eQMrOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Polynomial Regression\n",
        "\n",
        "Polynomial Regression digunakan ketika hubungan antara fitur dan target bersifat non-linear.\n",
        "Model ini memperluas Linear Regression dengan menambahkan fitur polinomial.\n"
      ],
      "metadata": {
        "id": "MB1iagEvIj50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Generating Nonlinear Data\n",
        "\n",
        "Untuk mengilustrasikan konsep Polynomial Regression, dibuat sebuah dataset sintetis yang memiliki hubungan non-linear antara fitur dan target. Dataset ini dirancang mengikuti pola kuadratik dengan tambahan noise acak agar menyerupai kondisi data dunia nyata.\n",
        "\n",
        "Hubungan antara variabel input dan target tidak lagi berbentuk garis lurus, sehingga model Linear Regression sederhana akan mengalami kesulitan dalam melakukan fitting secara optimal. Dataset semacam ini sangat sesuai untuk menunjukkan keunggulan Polynomial Regression dibandingkan regresi linier biasa.\n"
      ],
      "metadata": {
        "id": "KqrPTjDKNgCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        ""
      ],
      "metadata": {
        "id": "O011xDGsNv8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Polynomial Features\n",
        "\n",
        "Agar model linier mampu menangkap hubungan non-linear, fitur asli perlu diperluas menjadi fitur polinomial. Scikit-Learn menyediakan transformer **PolynomialFeatures** yang secara otomatis menghasilkan kombinasi fitur hingga derajat tertentu.\n",
        "\n",
        "Dengan menambahkan fitur polinomial, setiap instance tidak hanya direpresentasikan oleh fitur awal, tetapi juga oleh pangkat-pangkatnya. Pendekatan ini memungkinkan model untuk membentuk kurva non-linear meskipun algoritma yang digunakan tetap Linear Regression.\n",
        "\n",
        "Transformasi fitur ini merupakan inti dari Polynomial Regression, karena kompleksitas model berasal dari fitur, bukan dari algoritma pembelajarannya.\n"
      ],
      "metadata": {
        "id": "IdII-IUYNl6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "X_poly[0]"
      ],
      "metadata": {
        "id": "3qhvD8vMNzl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Polynomial Regression Model\n",
        "\n",
        "Setelah fitur polinomial terbentuk, proses pelatihan model dilakukan menggunakan Linear Regression seperti biasa. Model kemudian mempelajari koefisien untuk setiap fitur polinomial yang telah dihasilkan.\n",
        "\n",
        "Dengan adanya fitur kuadratik (atau derajat lebih tinggi), model mampu menyesuaikan diri terhadap pola non-linear pada data. Hal ini menunjukkan bahwa Polynomial Regression bukanlah algoritma baru, melainkan kombinasi antara transformasi fitur non-linear dan model regresi linier.\n",
        "\n",
        "Pendekatan ini banyak digunakan sebagai solusi sederhana namun efektif untuk memodelkan hubungan non-linear tanpa harus menggunakan algoritma yang lebih kompleks.\n",
        "\n"
      ],
      "metadata": {
        "id": "i9_ME5XzNqfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "lin_reg.coef_, lin_reg.intercept_"
      ],
      "metadata": {
        "id": "B7DpFmCZN2ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Underfitting and Overfitting\n",
        "\n",
        "Underfitting terjadi ketika model terlalu sederhana.\n",
        "Overfitting terjadi ketika model terlalu kompleks dan menangkap noise data.\n"
      ],
      "metadata": {
        "id": "NcWGF1txInlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Learning Curves\n",
        "\n",
        "Learning Curves digunakan untuk menganalisis performa model terhadap ukuran data training.\n",
        "Dengan learning curves, kita dapat mendeteksi apakah model mengalami:\n",
        "\n",
        "- **Underfitting** (bias tinggi)\n",
        "- **Overfitting** (variance tinggi)\n",
        "- atau sudah seimbang\n",
        "\n",
        "Learning curve menampilkan:\n",
        "- Error pada **training set**\n",
        "- Error pada **validation set**\n",
        "seiring bertambahnya jumlah data training.\n",
        "\n",
        "Jika:\n",
        "- Kedua kurva error tinggi dan berdekatan → **Underfitting**\n",
        "- Training error rendah tapi validation error tinggi → **Overfitting**\n",
        "- Kedua kurva rendah dan berdekatan → **Model baik**\n"
      ],
      "metadata": {
        "id": "_YLaeYUNKJ4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    LinearRegression(), X, y,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    cv=5,\n",
        "    scoring=\"neg_mean_squared_error\"\n",
        ")\n",
        "\n",
        "train_rmse = np.sqrt(-train_scores.mean(axis=1))\n",
        "val_rmse = np.sqrt(-val_scores.mean(axis=1))\n",
        "\n",
        "train_rmse, val_rmse\n",
        "\n"
      ],
      "metadata": {
        "id": "mFYDn0JUKKfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Regularized Linear Models\n",
        "\n",
        "Regularisasi digunakan untuk mengurangi overfitting dengan menambahkan penalti terhadap parameter model.\n"
      ],
      "metadata": {
        "id": "xoRTMMjTKZxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_reg = Ridge(alpha=1)\n",
        "ridge_reg.fit(X, y)\n"
      ],
      "metadata": {
        "id": "C7lnZgZ-Kamn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n"
      ],
      "metadata": {
        "id": "lVh04lQ5Kc0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Logistic Regression\n",
        "\n",
        "Logistic Regression digunakan untuk klasifikasi biner dengan menggunakan fungsi sigmoid.\n"
      ],
      "metadata": {
        "id": "JiXyUYYaKz0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Softmax Regression\n",
        "\n",
        "Softmax Regression adalah generalisasi Logistic Regression untuk multi-class classification.\n"
      ],
      "metadata": {
        "id": "uUA1pCIBK1pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Summary (Chapter 4)\n",
        "\n",
        "Chapter ini membahas proses pelatihan model Machine Learning secara menyeluruh, mulai dari Linear Regression hingga teknik optimisasi lanjutan.\n",
        "\n",
        "Pemahaman konsep Gradient Descent, Regularisasi, dan Logistic Regression menjadi fondasi penting sebelum memasuki topik neural networks dan deep learning.\n"
      ],
      "metadata": {
        "id": "NuDXgp5BK2MA"
      }
    }
  ]
}