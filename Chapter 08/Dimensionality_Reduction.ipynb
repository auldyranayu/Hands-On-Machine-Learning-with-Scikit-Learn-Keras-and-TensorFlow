{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 8: Dimensionality Reduction\n",
        "\n",
        "Dalam berbagai permasalahan Machine Learning, dataset sering kali memiliki jumlah fitur yang sangat besar. Kondisi ini menimbulkan tantangan serius, baik dari sisi komputasi maupun kualitas model.\n",
        "\n",
        "Dimensionality Reduction merupakan kumpulan teknik yang bertujuan mengurangi jumlah fitur tanpa menghilangkan terlalu banyak informasi penting. Teknik ini membantu mempercepat proses training, menurunkan risiko overfitting, serta mempermudah visualisasi data berdimensi tinggi.\n"
      ],
      "metadata": {
        "id": "ldQBsO9Fb_HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The Curse of Dimensionality\n",
        "\n",
        "Curse of dimensionality menggambarkan berbagai permasalahan yang muncul ketika jumlah dimensi data meningkat secara signifikan. Dalam ruang berdimensi tinggi, perilaku data menjadi sangat berbeda dibandingkan ruang dua atau tiga dimensi.\n"
      ],
      "metadata": {
        "id": "bZzq2sJPcDf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Sparsity in High-Dimensional Space\n",
        "\n",
        "Ketika dimensi bertambah, data menjadi semakin jarang (sparse), jarak antar titik meningkat, dan instance baru sering kali jauh dari seluruh data training. Kondisi ini menyulitkan model dalam melakukan generalisasi.\n"
      ],
      "metadata": {
        "id": "SNFyxCmgcHvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Dampak Curse of Dimensionality\n",
        "\n",
        "Beberapa dampak utama curse of dimensionality antara lain meningkatnya kebutuhan data training, kompleksitas model yang tinggi, serta penurunan performa prediksi.\n"
      ],
      "metadata": {
        "id": "P3FGR5vwcJ82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Main Approaches for Dimensionality Reduction\n",
        "\n",
        "Secara umum, dimensionality reduction dibagi menjadi dua pendekatan utama, yaitu projection dan manifold learning. Keduanya memiliki tujuan yang sama, tetapi menggunakan asumsi struktur data yang berbeda.\n"
      ],
      "metadata": {
        "id": "po95Qu-scK7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Manifold Learning\n",
        "\n",
        "Manifold learning berasumsi bahwa data terletak pada manifold non-linear berdimensi rendah. Pendekatan ini berfokus pada pelestarian hubungan lokal antar titik data.\n"
      ],
      "metadata": {
        "id": "HKWoRGGXcL40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Kapan Menggunakan Projection atau Manifold Learning?\n",
        "\n",
        "Projection umumnya digunakan sebagai preprocessing sebelum training model, sedangkan manifold learning lebih sering dimanfaatkan untuk visualisasi data.\n"
      ],
      "metadata": {
        "id": "CtDRCDM7cNJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) merupakan salah satu teknik dimensionality reduction yang paling banyak digunakan dalam bidang machine learning dan data science. PCA bertujuan untuk mereduksi jumlah dimensi data dengan cara mentransformasikan fitur-fitur asli ke dalam sekumpulan fitur baru yang saling ortogonal dan tersusun berdasarkan besarnya variansi yang mampu dijelaskan.\n",
        "\n",
        "Berbeda dengan feature selection yang hanya memilih sebagian fitur, PCA melakukan transformasi linier terhadap seluruh fitur sehingga menghasilkan representasi data yang lebih ringkas namun tetap mempertahankan informasi penting. Teknik ini sangat berguna ketika data memiliki banyak fitur yang saling berkorelasi.\n",
        "\n"
      ],
      "metadata": {
        "id": "JperOZKXcOeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Intuisi Dasar PCA\n",
        "\n",
        "Intuisi utama di balik PCA adalah mencari arah baru dalam ruang fitur di mana data memiliki penyebaran (variansi) terbesar. Arah ini disebut sebagai principal component pertama. Selanjutnya, PCA mencari arah kedua yang ortogonal terhadap arah pertama dan memiliki variansi terbesar berikutnya, dan seterusnya.\n",
        "\n",
        "Dengan memproyeksikan data ke beberapa principal components pertama, sebagian besar struktur dan pola data tetap dapat dipertahankan meskipun jumlah dimensi berkurang secara signifikan. Hal ini membuat PCA sangat efektif untuk visualisasi data berdimensi tinggi serta sebagai preprocessing sebelum pelatihan model machine learning.\n"
      ],
      "metadata": {
        "id": "Ck5DTpBweH7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Preserving Variance\n",
        "\n",
        "Salah satu tujuan utama PCA adalah mempertahankan sebanyak mungkin variansi data saat proses reduksi dimensi dilakukan. Variansi yang tinggi umumnya diasosiasikan dengan informasi yang relevan, sedangkan variansi yang rendah sering kali berkaitan dengan noise.\n",
        "\n",
        "Dengan memilih sejumlah principal components yang menjelaskan proporsi variansi tertentu (misalnya 90% atau 95%), PCA memastikan bahwa sebagian besar informasi penting dari data asli masih tersimpan dalam representasi berdimensi lebih rendah.\n"
      ],
      "metadata": {
        "id": "Rk5uZxbqeOEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 PCA vs Feature Selection\n",
        "\n",
        "PCA dan feature selection sering dianggap serupa, padahal keduanya memiliki pendekatan yang berbeda secara fundamental.\n",
        "\n",
        "Feature selection mempertahankan fitur asli dan hanya memilih subset yang dianggap paling relevan. Sebaliknya, PCA menciptakan fitur baru berupa kombinasi linier dari fitur asli. Akibatnya, PCA cenderung menghasilkan representasi yang lebih efisien, tetapi dengan konsekuensi berkurangnya interpretabilitas fitur.\n"
      ],
      "metadata": {
        "id": "T_dFEkoPePK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Keterbatasan PCA\n",
        "\n",
        "Meskipun sangat populer, PCA memiliki beberapa keterbatasan yang perlu diperhatikan. PCA hanya mampu menangkap hubungan linier antar fitur dan tidak efektif untuk pola non-linier. Selain itu, PCA sensitif terhadap skala fitur sehingga standardisasi data menjadi langkah penting sebelum penerapan PCA.\n",
        "\n",
        "PCA juga merupakan metode unsupervised, sehingga tidak mempertimbangkan label target dalam proses reduksi dimensi.\n"
      ],
      "metadata": {
        "id": "8CObvwfreQPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. PCA with NumPy and Scikit-Learn\n",
        "\n",
        "Untuk memahami PCA secara menyeluruh, penting untuk melihat bagaimana PCA bekerja secara matematis dan bagaimana penerapannya secara praktis menggunakan library machine learning.\n"
      ],
      "metadata": {
        "id": "PFyOsMd_eRLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 PCA dengan NumPy (Pendekatan Konseptual)\n",
        "\n",
        "Secara matematis, PCA dapat dihitung menggunakan Singular Value Decomposition (SVD). Langkah ini melibatkan dekomposisi matriks data yang telah dicenter sehingga menghasilkan principal components sebagai vektor eigen.\n"
      ],
      "metadata": {
        "id": "hdXYnV0WeSAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[2.5, 2.4],\n",
        "              [0.5, 0.7],\n",
        "              [2.2, 2.9],\n",
        "              [1.9, 2.2],\n",
        "              [3.1, 3.0]])\n",
        "\n",
        "X_centered = X - X.mean(axis=0)\n",
        "\n",
        "U, S, Vt = np.linalg.svd(X_centered)\n",
        "\n",
        "Vt\n"
      ],
      "metadata": {
        "id": "qhRK3YkQeS7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matriks Vt berisi principal components, di mana komponen pertama menunjukkan arah dengan variansi terbesar dalam data.\n"
      ],
      "metadata": {
        "id": "h61TWQVZeT6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Projecting Data onto Principal Components\n",
        "\n",
        "Setelah principal components diperoleh, data dapat diproyeksikan ke ruang berdimensi lebih rendah dengan mengalikan data terpusat dengan vektor komponen utama yang dipilih.\n"
      ],
      "metadata": {
        "id": "2-wU-JgweWcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = Vt[:1].T\n",
        "X_pca = X_centered.dot(W)\n",
        "X_pca\n"
      ],
      "metadata": {
        "id": "bEKLQwZzeU8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 PCA dengan Scikit-Learn\n",
        "\n",
        "Dalam praktik nyata, PCA umumnya diimplementasikan menggunakan Scikit-Learn karena lebih efisien dan stabil untuk dataset berukuran besar.\n"
      ],
      "metadata": {
        "id": "MlX4TR8zedwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "X_pca\n"
      ],
      "metadata": {
        "id": "JG2vIJSAefIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil proyeksi ini merepresentasikan data dalam satu dimensi yang mempertahankan sebagian besar variansi.\n"
      ],
      "metadata": {
        "id": "VKQ8b1zEebsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Choosing the Right Number of Dimensions\n",
        "\n",
        "Menentukan jumlah komponen utama merupakan keputusan penting. Jumlah komponen yang terlalu sedikit dapat menghilangkan informasi penting, sedangkan terlalu banyak mengurangi manfaat dimensionality reduction.\n"
      ],
      "metadata": {
        "id": "E7vz14AkenQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Explained Variance Ratio\n",
        "\n",
        "Explained variance ratio menunjukkan proporsi variansi yang dijelaskan oleh masing-masing principal component.\n"
      ],
      "metadata": {
        "id": "bTMh6Tn7ejzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "id": "RkO0_sk3eo8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Cumulative Explained Variance\n",
        "\n",
        "Cumulative explained variance membantu menentukan jumlah komponen minimum yang diperlukan untuk mempertahankan proporsi variansi tertentu.\n"
      ],
      "metadata": {
        "id": "0JG8dTVmep3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.cumsum(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "wiodY9Ileq2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Choosing Components Automatically\n",
        "\n",
        "Scikit-Learn memungkinkan pemilihan jumlah komponen secara otomatis berdasarkan target variansi yang ingin dipertahankan.\n"
      ],
      "metadata": {
        "id": "M3syCSV7erur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_95 = PCA(n_components=0.95)\n",
        "X_reduced = pca_95.fit_transform(X)\n",
        "\n",
        "X_reduced.shape\n"
      ],
      "metadata": {
        "id": "EnBFy41xeu6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. PCA Variants\n",
        "\n",
        "Untuk dataset besar, tersedia varian PCA yang lebih efisien seperti Randomized PCA dan Incremental PCA.\n"
      ],
      "metadata": {
        "id": "Hr1fV5WBchzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_randomized = PCA(n_components=2, svd_solver=\"randomized\", random_state=42)\n",
        "X_pca_randomized = pca_randomized.fit_transform(X)\n",
        "X_pca_randomized\n"
      ],
      "metadata": {
        "id": "QfewZzmrcjjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "inc_pca = IncrementalPCA(n_components=2)\n",
        "\n",
        "for batch in np.array_split(X, 3):\n",
        "    inc_pca.partial_fit(batch)\n",
        "\n",
        "X_pca_incremental = inc_pca.transform(X)\n",
        "X_pca_incremental\n"
      ],
      "metadata": {
        "id": "ipoeL5sBclKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Kernel PCA\n",
        "\n",
        "Principal Component Analysis (PCA) standar hanya mampu menangkap hubungan linier antar fitur. Pada banyak dataset dunia nyata, struktur data bersifat non-linear sehingga PCA biasa kurang efektif.\n",
        "\n",
        "Kernel PCA memperluas PCA dengan memanfaatkan *kernel trick*, sehingga PCA dapat diterapkan pada ruang fitur berdimensi lebih tinggi secara implisit tanpa perlu menghitung transformasi tersebut secara eksplisit.\n"
      ],
      "metadata": {
        "id": "EdXxb9Vzc-ET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Intuisi Kernel Trick\n",
        "\n",
        "Kernel trick memungkinkan algoritma linier bekerja pada data non-linear dengan cara menghitung kesamaan antar titik data menggunakan fungsi kernel.\n",
        "\n",
        "Alih-alih memetakan data secara eksplisit ke ruang berdimensi tinggi, kernel menghitung produk dalam ruang fitur tersebut secara langsung.\n",
        "\n",
        "Dengan pendekatan ini, hubungan non-linear di ruang asli dapat menjadi linier di ruang fitur hasil transformasi kernel.\n"
      ],
      "metadata": {
        "id": "owXMVsFXc_3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beberapa fungsi kernel yang umum digunakan dalam Kernel PCA antara lain:\n",
        "\n",
        "- Linear: setara dengan PCA standar\n",
        "- Polynomial: menangkap hubungan polinomial\n",
        "- RBF (Gaussian): sangat fleksibel untuk pola non-linear kompleks\n",
        "- Sigmoid: memiliki kemiripan dengan fungsi aktivasi pada neural network\n"
      ],
      "metadata": {
        "id": "4KLEfthUdBF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
        "X_kpca = kpca.fit_transform(X)\n",
        "X_kpca\n"
      ],
      "metadata": {
        "id": "SwIC7gNdc_WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Kernel PCA dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi Kernel PCA melalui class `KernelPCA`. Implementasi ini memungkinkan penggunaan berbagai kernel dan dapat disesuaikan dengan karakteristik data.\n",
        "\n",
        "Parameter `gamma` pada kernel RBF mengontrol lebar kernel dan memiliki pengaruh besar terhadap hasil transformasi.\n"
      ],
      "metadata": {
        "id": "Qef2NuXcdDZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Kernel PCA sebagai Preprocessing\n",
        "\n",
        "Kernel PCA sering digunakan sebagai tahap preprocessing sebelum algoritma Machine Learning lain seperti Support Vector Machine atau Logistic Regression.\n",
        "\n",
        "Namun, karena transformasi kernel bersifat non-linear dan tidak selalu dapat diinversi, interpretasi fitur hasil Kernel PCA menjadi lebih sulit dibandingkan PCA standar.\n"
      ],
      "metadata": {
        "id": "PN-q-SkLdEcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Locally Linear Embedding (LLE)\n",
        "\n",
        "Locally Linear Embedding (LLE) merupakan algoritma dimensionality reduction berbasis manifold learning yang dirancang untuk mempertahankan struktur lokal data.\n",
        "\n",
        "Berbeda dengan PCA yang bersifat linier, LLE mampu menangani struktur non-linear dengan lebih baik.\n"
      ],
      "metadata": {
        "id": "fzLa9yOBdFeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Intuisi Dasar LLE\n",
        "\n",
        "LLE berasumsi bahwa setiap titik data dapat direpresentasikan sebagai kombinasi linier dari tetangga terdekatnya.\n",
        "\n",
        "Jika hubungan lokal ini dapat dipertahankan dalam ruang berdimensi lebih rendah, maka struktur manifold data juga tetap terjaga.\n"
      ],
      "metadata": {
        "id": "jK3NKhDGdGW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara umum, algoritma LLE bekerja melalui tiga tahap utama:\n",
        "\n",
        "1. Menentukan tetangga terdekat (k-nearest neighbors) untuk setiap titik data\n",
        "2. Menghitung bobot rekonstruksi setiap titik dari tetangganya\n",
        "3. Mencari embedding berdimensi rendah yang mempertahankan bobot tersebut\n"
      ],
      "metadata": {
        "id": "Arz75sbFdHpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Karakteristik LLE\n",
        "\n",
        "Beberapa karakteristik penting dari LLE meliputi:\n",
        "\n",
        "- Sangat efektif untuk visualisasi data berdimensi tinggi\n",
        "- Mampu menangkap struktur manifold non-linear\n",
        "- Sensitif terhadap noise dan pemilihan jumlah tetangga\n",
        "\n",
        "Karena fokus pada struktur lokal, LLE jarang digunakan sebagai preprocessing sebelum training model Machine Learning.\n"
      ],
      "metadata": {
        "id": "QC4lXIsjdJGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_lle = lle.fit_transform(X)\n",
        "X_lle\n"
      ],
      "metadata": {
        "id": "zEpOKyCsdJ_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Other Dimensionality Reduction Techniques\n",
        "\n",
        "Selain PCA dan LLE, terdapat berbagai teknik dimensionality reduction lain yang sering digunakan, terutama untuk eksplorasi dan visualisasi data berdimensi tinggi.\n"
      ],
      "metadata": {
        "id": "8T-PiccXdL7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Multidimensional Scaling (MDS)\n",
        "\n",
        "Multidimensional Scaling (MDS) bertujuan merepresentasikan data dalam ruang berdimensi rendah dengan mempertahankan jarak antar titik data.\n",
        "\n",
        "MDS sering digunakan ketika informasi jarak antar instance memiliki peran penting dalam analisis data.\n"
      ],
      "metadata": {
        "id": "Jfk65agrdMzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "t-SNE merupakan teknik manifold learning yang sangat populer untuk visualisasi data berdimensi tinggi.\n",
        "\n",
        "Algoritma ini sangat baik dalam mempertahankan struktur lokal dan sering digunakan untuk memvisualisasikan clustering data.\n",
        "\n",
        "Namun, t-SNE memiliki beberapa keterbatasan, seperti mahal secara komputasi dan sensitif terhadap hyperparameter.\n"
      ],
      "metadata": {
        "id": "NEKq-gXPdN8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Isomap\n",
        "\n",
        "Isomap merupakan metode manifold learning yang mempertahankan jarak geodesik antar titik data pada manifold.\n",
        "\n",
        "Metode ini cocok untuk data dengan struktur non-linear yang relatif halus, tetapi kurang robust terhadap noise.\n"
      ],
      "metadata": {
        "id": "jty33UY7dO0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Summary (Chapter 8)\n",
        "\n",
        "Chapter ini membahas berbagai teknik dimensionality reduction yang bertujuan mengurangi kompleksitas data tanpa menghilangkan terlalu banyak informasi penting.\n",
        "\n",
        "Topik utama yang dipelajari meliputi curse of dimensionality, PCA dan variannya, Kernel PCA, serta metode manifold learning seperti LLE, MDS, t-SNE, dan Isomap.\n",
        "\n",
        "Pemahaman dimensionality reduction sangat penting dalam Machine Learning modern, baik untuk meningkatkan efisiensi model maupun untuk memahami struktur data kompleks.\n"
      ],
      "metadata": {
        "id": "USah2_RhdP-Z"
      }
    }
  ]
}