{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10: Introduction to Artificial Neural Networks with Keras\n",
        "\n",
        "Pada chapter ini, pembahasan mulai beralih dari Machine Learning klasik menuju paradigma **Deep Learning**, dengan fokus utama pada **Artificial Neural Networks (ANN)**.\n",
        "\n",
        "Artificial Neural Networks merupakan model yang terinspirasi oleh sistem saraf biologis manusia, namun dirancang secara matematis dan komputasional agar mampu mempelajari pola kompleks dari data.\n",
        "\n",
        "Chapter ini berfungsi sebagai fondasi penting sebelum mempelajari arsitektur deep learning yang lebih dalam, seperti Convolutional Neural Networks (CNN) dan Recurrent Neural Networks (RNN).\n",
        "\n",
        "Topik utama yang akan dibahas meliputi:\n",
        "- asal-usul dan intuisi jaringan saraf tiruan,\n",
        "- neuron artifisial dan perceptron,\n",
        "- Multilayer Perceptron (MLP),\n",
        "- algoritma backpropagation,\n",
        "- fungsi aktivasi,\n",
        "- serta implementasi ANN menggunakan Keras.\n"
      ],
      "metadata": {
        "id": "dhPHyJ7aVxa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dari Neuron Biologis ke Neuron Artifisial\n",
        "\n",
        "Otak manusia terdiri dari jaringan neuron biologis yang sangat besar dan kompleks. Setiap neuron menerima sinyal listrik dari neuron lain, memproses sinyal tersebut, dan meneruskannya ke neuron berikutnya.\n",
        "\n",
        "Meskipun satu neuron biologis relatif sederhana, kombinasi miliaran neuron mampu menghasilkan kemampuan luar biasa seperti:\n",
        "- pengenalan pola,\n",
        "- pemrosesan bahasa,\n",
        "- pengambilan keputusan,\n",
        "- dan pembelajaran adaptif.\n",
        "\n",
        "Artificial Neural Networks tidak bertujuan meniru otak secara biologis, melainkan meniru **konsep pemrosesan informasinya** secara abstrak.\n"
      ],
      "metadata": {
        "id": "li8knFVtV2x4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN modern lebih menekankan pada kemampuan:\n",
        "- belajar dari data,\n",
        "- melakukan generalisasi,\n",
        "- dan menyelesaikan masalah kompleks.\n",
        "\n",
        "Pendekatan ini serupa dengan pesawat terbang yang terinspirasi dari burung, tetapi menggunakan prinsip aerodinamika, bukan kepakan sayap biologis.\n"
      ],
      "metadata": {
        "id": "X9ylkInfV4Qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neuron Artifisial Awal (McCulloch–Pitts Neuron)\n",
        "\n",
        "Model neuron artifisial pertama diperkenalkan oleh McCulloch dan Pitts pada tahun 1943.\n",
        "\n",
        "Neuron ini memiliki karakteristik:\n",
        "- input biner (0 atau 1),\n",
        "- output biner,\n",
        "- sebuah ambang batas aktivasi.\n",
        "\n",
        "Neuron akan aktif jika jumlah input aktif melebihi ambang batas tertentu.\n",
        "\n",
        "Walaupun sangat sederhana, model ini mampu merepresentasikan operasi logika dasar seperti AND, OR, dan NOT.\n"
      ],
      "metadata": {
        "id": "_Kw8e9qsV6Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model McCulloch–Pitts menunjukkan bahwa dengan menggabungkan unit-unit sederhana, sistem yang lebih kompleks dapat dibangun.\n",
        "\n",
        "Konsep ini menjadi dasar pemikiran bahwa **kompleksitas dapat muncul dari struktur**, bukan dari kecanggihan satu unit tunggal.\n"
      ],
      "metadata": {
        "id": "b6Q-NR5YV8H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Perceptron\n",
        "\n",
        "Perceptron diperkenalkan oleh Frank Rosenblatt pada tahun 1957 sebagai model pembelajaran berbasis neuron artifisial.\n",
        "\n",
        "Perceptron menggunakan unit yang disebut **Threshold Logic Unit (TLU)**, yang bekerja dengan cara:\n",
        "- menerima input numerik,\n",
        "- mengalikan input dengan bobot,\n",
        "- menjumlahkannya,\n",
        "- lalu menerapkan fungsi ambang (step function).\n"
      ],
      "metadata": {
        "id": "9W8d1F5ZV9Wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Secara matematis, perceptron menghitung:\n",
        "\n",
        "$$\n",
        "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "$$\n",
        "\n",
        "Kemudian menghasilkan output:\n",
        "\n",
        "- 1 jika $z \\geq 0$\n",
        "- 0 jika $z < 0$\n",
        "\n",
        "Perceptron dapat digunakan untuk klasifikasi linier, mirip dengan Logistic Regression atau Linear SVM.\n"
      ],
      "metadata": {
        "id": "GaeghehfV-wD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training a Perceptron\n",
        "\n",
        "Keunggulan utama perceptron adalah adanya aturan pembelajaran yang sederhana dan intuitif.\n",
        "\n",
        "Tujuan training adalah mencari bobot dan bias yang mampu memisahkan data ke dalam kelas yang benar.\n"
      ],
      "metadata": {
        "id": "7Og5amROWCnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Perceptron Learning Rule\n",
        "\n",
        "Aturan pembelajaran perceptron memperbarui bobot hanya ketika terjadi kesalahan prediksi.\n",
        "\n",
        "Rumus pembaruan bobot:\n",
        "\n",
        "$$\n",
        "w_i \\leftarrow w_i + \\eta (y - \\hat{y}) x_i\n",
        "$$\n",
        "\n",
        "di mana:\n",
        "- $\\eta$ adalah learning rate,\n",
        "- $y$ adalah label sebenarnya,\n",
        "- $\\hat{y}$ adalah prediksi model.\n"
      ],
      "metadata": {
        "id": "htewuNzVWEPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuisi dari aturan ini adalah menggeser decision boundary agar kesalahan klasifikasi semakin berkurang pada iterasi berikutnya.\n"
      ],
      "metadata": {
        "id": "plWuCFq6WGpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Konvergensi Perceptron\n",
        "\n",
        "Perceptron memiliki jaminan konvergensi jika data bersifat linearly separable.\n",
        "\n",
        "Namun, jika data tidak dapat dipisahkan secara linier, perceptron tidak akan pernah konvergen.\n"
      ],
      "metadata": {
        "id": "qgUgAMCqWI4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Implementasi Perceptron dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi perceptron yang memudahkan eksperimen dan pembelajaran konsep dasar neural network.\n"
      ],
      "metadata": {
        "id": "hmScW3LxWLNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)]\n",
        "y = (iris.target == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=42\n",
        ")\n",
        "\n",
        "perceptron = Perceptron(random_state=42)\n",
        "perceptron.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "29bqQqEDWNDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron di atas digunakan untuk memisahkan kelas Iris Setosa dari kelas lainnya.\n",
        "\n",
        "Model ini hanya mampu membentuk decision boundary linier.\n"
      ],
      "metadata": {
        "id": "vLyDGyxPWO-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Keterbatasan Perceptron\n",
        "\n",
        "Perceptron memiliki keterbatasan utama:\n",
        "- hanya menangani pola linier,\n",
        "- gagal menyelesaikan masalah XOR,\n",
        "- tidak menghasilkan probabilitas output.\n",
        "\n",
        "Keterbatasan ini mendorong pengembangan jaringan saraf multilayer.\n"
      ],
      "metadata": {
        "id": "HDjaagzEWRd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Multilayer Perceptron (MLP)\n",
        "\n",
        "Multilayer Perceptron dikembangkan untuk mengatasi keterbatasan perceptron tunggal.\n",
        "\n",
        "MLP terdiri dari:\n",
        "- input layer,\n",
        "- satu atau lebih hidden layer,\n",
        "- output layer.\n",
        "\n",
        "Dengan fungsi aktivasi non-linear, MLP mampu mempelajari pola non-linear.\n"
      ],
      "metadata": {
        "id": "hTrG9-BiWTrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Peran Hidden Layer\n",
        "\n",
        "Hidden layer memungkinkan jaringan membangun representasi bertingkat.\n",
        "\n",
        "Layer awal menangkap pola sederhana, sedangkan layer yang lebih dalam menangkap pola kompleks.\n"
      ],
      "metadata": {
        "id": "X1Twxt02WVKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Backpropagation\n",
        "\n",
        "Backpropagation adalah algoritma inti untuk melatih MLP.\n",
        "\n",
        "Algoritma ini menghitung gradien error terhadap setiap bobot menggunakan aturan rantai dari kalkulus.\n"
      ],
      "metadata": {
        "id": "LTQGwnnmWWzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Intuisi Backpropagation\n",
        "\n",
        "Backpropagation terdiri dari:\n",
        "1. Forward pass untuk menghitung output\n",
        "2. Backward pass untuk menyebarkan error dan memperbarui bobot\n"
      ],
      "metadata": {
        "id": "W6wZH-zIWYv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Universal Approximation Theorem\n",
        "\n",
        "Teorema ini menyatakan bahwa MLP dengan satu hidden layer yang cukup besar mampu mendekati fungsi kontinu apa pun.\n",
        "\n",
        "Namun, dalam praktik, jaringan dengan banyak layer lebih efisien dan stabil.\n"
      ],
      "metadata": {
        "id": "dg_vv0s2Wawn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Activation Functions\n",
        "\n",
        "Fungsi aktivasi merupakan komponen **paling krusial** dalam Artificial Neural Networks (ANN).\n",
        "Tanpa fungsi aktivasi non-linear, jaringan saraf yang terdiri dari banyak layer **tidak akan lebih kuat daripada model linear sederhana**.\n",
        "\n",
        "Fungsi aktivasi menentukan bagaimana sinyal hasil kombinasi linear dari input akan diubah menjadi output neuron.\n",
        "Dengan kata lain, fungsi aktivasi menentukan:\n",
        "- apakah neuron akan aktif,\n",
        "- seberapa besar sinyal yang diteruskan ke layer berikutnya,\n",
        "- dan bagaimana jaringan mempelajari pola kompleks.\n",
        "\n",
        "Tanpa fungsi aktivasi non-linear, beberapa layer Dense akan setara dengan satu layer linear saja.\n"
      ],
      "metadata": {
        "id": "df5MtuahX2Jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Sigmoid Function\n",
        "\n",
        "Sigmoid merupakan salah satu fungsi aktivasi **tertua** dalam neural network.\n",
        "Fungsi ini memetakan nilai input ke rentang **(0, 1)**, sehingga sering diinterpretasikan sebagai probabilitas.\n",
        "\n",
        "Secara matematis, sigmoid didefinisikan sebagai:\n",
        "\n",
        "\\[\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\]\n",
        "\n",
        "Karakteristik sigmoid:\n",
        "- output berada di antara 0 dan 1,\n",
        "- cocok untuk klasifikasi biner,\n",
        "- sering digunakan pada output layer model klasik.\n"
      ],
      "metadata": {
        "id": "Lz3iYNfVX4kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Namun, sigmoid memiliki beberapa kelemahan serius:\n",
        "- **Vanishing gradient** pada nilai ekstrem (z sangat besar atau sangat kecil),\n",
        "- output tidak berpusat di nol sehingga memperlambat konvergensi,\n",
        "- tidak cocok untuk jaringan dalam (deep neural network).\n",
        "\n",
        "Karena alasan ini, sigmoid **jarang digunakan pada hidden layer** modern, tetapi masih relevan untuk output layer klasifikasi biner.\n"
      ],
      "metadata": {
        "id": "EzgvCSjJX6FH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 ReLU (Rectified Linear Unit)\n",
        "\n",
        "ReLU adalah fungsi aktivasi paling populer dalam deep learning modern.\n",
        "Fungsi ini sederhana dan sangat efisien secara komputasi.\n",
        "\n",
        "Definisi ReLU:\n",
        "\n",
        "\\[\n",
        "\\text{ReLU}(z) = \\max(0, z)\n",
        "\\]\n",
        "\n",
        "Artinya:\n",
        "- jika z > 0 → output = z\n",
        "- jika z ≤ 0 → output = 0\n"
      ],
      "metadata": {
        "id": "mYCulpI9X9ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kelebihan ReLU:\n",
        "- mengurangi masalah vanishing gradient,\n",
        "- mempercepat proses training,\n",
        "- komputasi sangat cepat,\n",
        "- bekerja sangat baik pada jaringan dalam.\n",
        "\n",
        "Kelemahan ReLU:\n",
        "- **Dying ReLU**, yaitu neuron bisa berhenti aktif jika bobotnya terus menghasilkan nilai negatif.\n",
        "\n",
        "Walaupun memiliki kelemahan, ReLU tetap menjadi pilihan default untuk hidden layer.\n"
      ],
      "metadata": {
        "id": "fI2h6zuNX-u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Softmax Function\n",
        "\n",
        "Softmax digunakan hampir secara eksklusif pada **output layer untuk klasifikasi multikelas**.\n",
        "Fungsi ini mengubah vektor skor menjadi **distribusi probabilitas** yang jumlahnya sama dengan 1.\n",
        "\n",
        "Definisi softmax:\n",
        "\n",
        "\\[\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "\\]\n",
        "\n",
        "Output softmax memudahkan interpretasi hasil model sebagai probabilitas setiap kelas.\n"
      ],
      "metadata": {
        "id": "KdWXQjqoYASW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax sangat cocok digunakan ketika:\n",
        "- jumlah kelas lebih dari dua,\n",
        "- hanya satu kelas yang benar (mutually exclusive),\n",
        "- ingin interpretasi probabilistik dari output model.\n"
      ],
      "metadata": {
        "id": "wEXuoBJ3YGSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Membangun Multilayer Perceptron dengan Keras\n",
        "\n",
        "Setelah memahami konsep neuron, layer, backpropagation, dan fungsi aktivasi,\n",
        "langkah berikutnya adalah **mengimplementasikan Multilayer Perceptron (MLP)** secara praktis.\n",
        "\n",
        "Framework yang digunakan adalah **Keras**, yang merupakan API tingkat tinggi dari TensorFlow.\n",
        "Keras memudahkan pembangunan, pelatihan, dan evaluasi neural network.\n"
      ],
      "metadata": {
        "id": "J9rJukWsYIUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Dataset MNIST\n",
        "\n",
        "Dataset MNIST berisi gambar tulisan tangan angka 0–9.\n",
        "Setiap gambar berukuran 28×28 piksel dalam bentuk grayscale.\n",
        "\n",
        "MNIST sering digunakan sebagai contoh awal deep learning karena:\n",
        "- ukurannya relatif kecil,\n",
        "- cukup kompleks untuk menunjukkan kekuatan neural network,\n",
        "- mudah digunakan.\n"
      ],
      "metadata": {
        "id": "4HE5Wqw_YJnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n"
      ],
      "metadata": {
        "id": "28YTHBk4YLKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset ini terdiri dari:\n",
        "- X_train dan y_train untuk training,\n",
        "- X_test dan y_test untuk testing.\n",
        "\n",
        "Nilai piksel berada pada rentang 0–255, sehingga perlu dilakukan preprocessing.\n"
      ],
      "metadata": {
        "id": "6cZGqclhYNMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Preprocessing Data\n",
        "\n",
        "Preprocessing sangat penting agar training neural network stabil dan cepat.\n",
        "Langkah utama adalah normalisasi nilai piksel ke rentang [0, 1].\n",
        "\n",
        "Normalisasi dilakukan dengan membagi nilai piksel dengan 255.\n"
      ],
      "metadata": {
        "id": "b3EIGpeWYOLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n"
      ],
      "metadata": {
        "id": "KVY6bEJtYTge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Membangun Arsitektur Model\n",
        "\n",
        "Model MLP akan terdiri dari:\n",
        "- Flatten layer untuk mengubah input 2D menjadi vektor 1D,\n",
        "- dua hidden layer dengan aktivasi ReLU,\n",
        "- output layer dengan aktivasi Softmax.\n"
      ],
      "metadata": {
        "id": "a2X1vjzzYWDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "f99ONUsZYXxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arsitektur ini cukup sederhana namun sangat efektif untuk MNIST.\n",
        "Jumlah neuron dapat disesuaikan sesuai kompleksitas data.\n"
      ],
      "metadata": {
        "id": "QWRyCGOIYZcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Compiling the Neural Network\n",
        "\n",
        "Sebelum training, model harus di-compile.\n",
        "Proses compile menentukan:\n",
        "- loss function,\n",
        "- optimizer,\n",
        "- metrics evaluasi.\n"
      ],
      "metadata": {
        "id": "gvfSWKTxYbEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "RUm1A4TQYcau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function dipilih sesuai format label integer.\n",
        "Optimizer SGD digunakan untuk menunjukkan konsep dasar optimisasi.\n"
      ],
      "metadata": {
        "id": "HHZMTnPVYeh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Training the Model\n",
        "\n",
        "Training dilakukan menggunakan method `fit()`.\n",
        "Pada tahap ini, model akan:\n",
        "- melakukan forward pass,\n",
        "- menghitung loss,\n",
        "- melakukan backpropagation,\n",
        "- memperbarui bobot.\n"
      ],
      "metadata": {
        "id": "fJ4AlgQNYgAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    validation_split=0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "v4F5BZWEYiL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation set digunakan untuk memantau overfitting selama training.\n"
      ],
      "metadata": {
        "id": "hIuUv8EGYmqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "royMr5mSYocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluasi ini menunjukkan kemampuan generalisasi model.\n"
      ],
      "metadata": {
        "id": "FzxwIx-TYqDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. Making Predictions with the Trained Model\n",
        "\n",
        "Model yang telah dilatih dapat digunakan untuk membuat prediksi pada data baru.\n",
        "Output berupa probabilitas dari setiap kelas.\n"
      ],
      "metadata": {
        "id": "LrNBFBdlYrTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = model.predict(X_test[:5])\n",
        "y_pred = y_proba.argmax(axis=1)\n",
        "\n",
        "y_pred\n"
      ],
      "metadata": {
        "id": "ZIr2rniMYuQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediksi diperoleh dengan mengambil kelas dengan probabilitas tertinggi.\n"
      ],
      "metadata": {
        "id": "TMYEjBbUYtjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Summary (Chapter 10)\n",
        "\n",
        "Pada chapter ini, kita mempelajari:\n",
        "- fungsi aktivasi utama (Sigmoid, ReLU, Softmax),\n",
        "- membangun Multilayer Perceptron dengan Keras,\n",
        "- preprocessing data,\n",
        "- training, evaluasi, dan prediksi model.\n",
        "\n",
        "Chapter ini menjadi fondasi utama untuk memahami Deep Neural Networks\n",
        "dan menjadi pintu masuk ke arsitektur yang lebih kompleks pada chapter berikutnya.\n"
      ],
      "metadata": {
        "id": "Y-MTFW6mYxZU"
      }
    }
  ]
}