{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "\n",
        "Pada bab ini, kita membahas pendekatan lanjutan dalam **Natural Language Processing (NLP)**\n",
        "dengan memanfaatkan **Recurrent Neural Networks (RNN)** dan mekanisme **Attention**.\n",
        "Pemrosesan bahasa alami memiliki tantangan tersendiri karena bahasa manusia\n",
        "mengandung konteks, makna implisit, serta ketergantungan jangka panjang antar kata.\n",
        "\n",
        "Bab ini memperkenalkan transisi penting dari model berbasis urutan klasik\n",
        "menuju arsitektur modern seperti **Transformer**, yang saat ini menjadi standar\n",
        "dalam berbagai aplikasi NLP skala besar.\n"
      ],
      "metadata": {
        "id": "3kq8B3uYyd77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tujuan Pembelajaran\n",
        "\n",
        "Setelah mempelajari bab ini, pembaca diharapkan mampu:\n",
        "1. Memahami konsep **Char-RNN** untuk generasi teks.\n",
        "2. Menggunakan **Embedding Layer** dalam tugas analisis sentimen.\n",
        "3. Menjelaskan arsitektur **Encoder–Decoder** pada terjemahan mesin.\n",
        "4. Memahami cara kerja **Attention Mechanism**.\n",
        "5. Mengenali peran **Transformer** dalam NLP modern.\n"
      ],
      "metadata": {
        "id": "jM9BY3hLyfmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Menghasilkan Teks Karakter demi Karakter (Char-RNN)\n",
        "\n",
        "Char-RNN adalah pendekatan generatif yang memprediksi karakter berikutnya\n",
        "berdasarkan urutan karakter sebelumnya. Model ini tidak memerlukan\n",
        "pemahaman kata secara eksplisit, melainkan mempelajari pola bahasa\n",
        "langsung dari urutan karakter.\n",
        "\n",
        "Pendekatan ini sangat berguna untuk mempelajari struktur bahasa,\n",
        "meskipun memerlukan data dan waktu pelatihan yang relatif besar\n",
        "untuk menghasilkan teks yang koheren.\n"
      ],
      "metadata": {
        "id": "N7q3Qbqzygu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Tokenisasi tingkat karakter\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "sample_text = \"To be, or not to be, that is the question.\"\n",
        "tokenizer.fit_on_texts([sample_text])\n",
        "\n",
        "max_id = len(tokenizer.word_index)\n",
        "encoded_text = tokenizer.texts_to_sequences([sample_text])[0]\n",
        "\n",
        "print(\"Jumlah karakter unik:\", max_id)\n",
        "print(\"Panjang teks:\", len(encoded_text))\n",
        "\n",
        "# Model Char-RNN berbasis GRU\n",
        "model_char_rnn = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(\n",
        "        keras.layers.Dense(max_id, activation=\"softmax\")\n",
        "    )\n",
        "])\n",
        "\n",
        "model_char_rnn.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\"\n",
        ")\n",
        "\n",
        "model_char_rnn.summary()\n"
      ],
      "metadata": {
        "id": "osFrfda6yhr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Stateful vs Stateless RNN\n",
        "\n",
        "RNN dapat dikategorikan berdasarkan cara mereka\n",
        "menangani *hidden state* antar batch:\n",
        "\n",
        "| Aspek | Stateless RNN | Stateful RNN |\n",
        "|------|---------------|--------------|\n",
        "| Hidden state | Di-reset setiap batch | Dipertahankan |\n",
        "| Urutan data | Dapat diacak | Harus berurutan |\n",
        "| Konteks | Jangka pendek | Jangka panjang |\n",
        "\n",
        "Stateful RNN sangat efektif untuk data berurutan panjang,\n",
        "namun membutuhkan pengelolaan data yang lebih ketat\n",
        "selama proses pelatihan.\n"
      ],
      "metadata": {
        "id": "47zXPa7Eyi4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Analisis Sentimen\n",
        "\n",
        "Analisis sentimen bertujuan untuk menentukan opini\n",
        "positif atau negatif dari sebuah teks. Dalam NLP modern,\n",
        "kata-kata diubah menjadi vektor padat menggunakan\n",
        "**Embedding Layer**, yang mampu merepresentasikan\n",
        "hubungan semantik antar kata.\n",
        "\n",
        "Penggunaan **Masking** memastikan bahwa token padding\n",
        "tidak mempengaruhi proses pembelajaran model.\n"
      ],
      "metadata": {
        "id": "tUfeCLpFykeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sentiment = keras.models.Sequential([\n",
        "    keras.layers.Embedding(\n",
        "        input_dim=10000,\n",
        "        output_dim=128,\n",
        "        mask_zero=True\n",
        "    ),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_sentiment.compile(\n",
        "    loss=\"binary_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model_sentiment.summary()\n"
      ],
      "metadata": {
        "id": "Vu7QkZ7OylhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encoder–Decoder untuk Terjemahan Mesin\n",
        "\n",
        "Model Encoder–Decoder terdiri dari dua komponen utama:\n",
        "Encoder memproses kalimat sumber dan mengubahnya\n",
        "menjadi representasi vektor, sedangkan Decoder\n",
        "menghasilkan kalimat target secara bertahap.\n",
        "\n",
        "Keterbatasan utama pendekatan ini adalah\n",
        "ketergantungan pada satu vektor konteks,\n",
        "yang menjadi kurang efektif untuk kalimat panjang.\n"
      ],
      "metadata": {
        "id": "c3zn6xiZymoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Mekanisme Attention\n",
        "\n",
        "Attention memungkinkan model untuk memfokuskan\n",
        "perhatiannya pada bagian tertentu dari input\n",
        "yang paling relevan saat menghasilkan output.\n",
        "\n",
        "Dengan attention, decoder tidak lagi bergantung\n",
        "pada satu vektor ringkasan, melainkan dapat\n",
        "mengakses seluruh output encoder.\n"
      ],
      "metadata": {
        "id": "bUFOOS7tynws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Transformer\n",
        "\n",
        "Transformer adalah arsitektur yang sepenuhnya\n",
        "mengandalkan self-attention tanpa menggunakan RNN.\n",
        "Pendekatan ini memungkinkan pelatihan yang\n",
        "lebih cepat dan skalabel.\n",
        "\n",
        "Komponen utama Transformer:\n",
        "- Multi-Head Attention\n",
        "- Positional Encoding\n",
        "- Residual Connections\n"
      ],
      "metadata": {
        "id": "LvliJ7fZyopK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh Multi-Head Attention\n",
        "mha_layer = keras.layers.MultiHeadAttention(num_heads=8, key_dim=64)\n",
        "\n",
        "dummy_input = tf.random.uniform((32, 50, 512))\n",
        "attention_output = mha_layer(dummy_input, dummy_input)\n",
        "\n",
        "print(\"Output shape:\", attention_output.shape)\n"
      ],
      "metadata": {
        "id": "O6LwtmOgypuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Bahasa Modern\n",
        "\n",
        "BERT dan GPT adalah contoh model bahasa modern\n",
        "berbasis Transformer.\n",
        "\n",
        "- **BERT** unggul dalam pemahaman konteks dua arah.\n",
        "- **GPT** unggul dalam generasi teks secara autoregresif.\n",
        "\n",
        "Keduanya menjadi fondasi utama dalam berbagai\n",
        "aplikasi NLP modern.\n"
      ],
      "metadata": {
        "id": "xC7Vp3mOyqqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ringkasan Bab\n",
        "\n",
        "1. NLP memerlukan model yang mampu memahami konteks urutan.\n",
        "2. Char-RNN memungkinkan generasi teks berbasis karakter.\n",
        "3. Embedding dan Attention adalah komponen kunci NLP modern.\n",
        "4. Transformer telah menggantikan RNN pada banyak tugas NLP.\n"
      ],
      "metadata": {
        "id": "tCsZb_QPyr70"
      }
    }
  ]
}