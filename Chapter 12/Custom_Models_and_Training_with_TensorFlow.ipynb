{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12: Custom Models and Training with TensorFlow\n",
        "\n",
        "Sebagian besar pengembangan model deep learning dapat diselesaikan menggunakan API tingkat tinggi seperti `tf.keras`. Namun, pada skenario lanjutan—terutama dalam penelitian dan eksperimen arsitektur baru—pendekatan tersebut sering kali kurang fleksibel.\n",
        "\n",
        "Chapter ini membahas bagaimana TensorFlow memungkinkan kita untuk bekerja pada level yang lebih rendah, sehingga pengembang dapat membangun komponen neural network secara manual. Dengan pendekatan ini, kita memperoleh kontrol penuh terhadap:\n",
        "- Cara model menghitung loss\n",
        "- Bagaimana bobot diperbarui\n",
        "- Struktur internal layer dan model\n",
        "\n",
        "Pemahaman ini sangat penting bagi praktisi deep learning yang ingin mengimplementasikan metode baru yang belum tersedia dalam API standar.\n"
      ],
      "metadata": {
        "id": "o9CTzEBBffPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ruang Lingkup Pembahasan Chapter 12\n",
        "\n",
        "Topik-topik utama yang akan dibahas pada bab ini meliputi:\n",
        "\n",
        "1. Penggunaan TensorFlow sebagai alternatif NumPy untuk komputasi numerik  \n",
        "2. Pembuatan fungsi loss kustom sesuai kebutuhan model  \n",
        "3. Implementasi custom layer dan custom model menggunakan subclassing  \n",
        "4. Mekanisme automatic differentiation melalui GradientTape  \n",
        "5. Penulisan training loop manual untuk kendali penuh proses pelatihan  \n",
        "6. Optimasi performa dengan TensorFlow Functions dan computational graph\n",
        "\n",
        "Seluruh konsep ini merupakan fondasi penting dalam pengembangan model deep learning tingkat lanjut.\n"
      ],
      "metadata": {
        "id": "9C_3RjSwfhLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Menggunakan TensorFlow seperti NumPy\n",
        "\n",
        "TensorFlow menyediakan struktur data bernama *Tensor* yang secara konseptual mirip dengan array NumPy. Perbedaannya terletak pada kemampuan TensorFlow untuk:\n",
        "- Menjalankan komputasi pada GPU atau TPU\n",
        "- Mendukung komputasi terdistribusi\n",
        "- Menghitung gradien secara otomatis\n",
        "\n",
        "Tensor dapat bersifat immutable (`tf.constant`) maupun mutable (`tf.Variable`). Dalam konteks neural network, bobot model selalu direpresentasikan menggunakan `tf.Variable` karena nilainya berubah selama proses training.\n"
      ],
      "metadata": {
        "id": "w_kucCgWfiIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Membuat Tensor\n",
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "print(\"Tensor:\\n\", t)\n",
        "\n",
        "# Operasi Dasar\n",
        "print(\"Tambah 10:\\n\", t + 10)\n",
        "print(\"Square:\\n\", tf.square(t))\n",
        "\n",
        "# Variabel (Mutable - digunakan untuk bobot model)\n",
        "v = tf.Variable([[1., 2.], [3., 4.]])\n",
        "v.assign(2 * v)\n",
        "v[0, 1].assign(42)\n",
        "print(\"Variable:\\n\", v)\n",
        "\n"
      ],
      "metadata": {
        "id": "mkEBZoCUfxqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Custom Loss Functions\n",
        "\n",
        "Fungsi loss berperan sebagai indikator utama performa model selama training. Dalam beberapa kasus, fungsi loss bawaan Keras tidak sepenuhnya sesuai dengan kebutuhan permasalahan.\n",
        "\n",
        "Dengan mendefinisikan fungsi loss sendiri, kita dapat:\n",
        "- Mengontrol sensitivitas terhadap outlier\n",
        "- Menggabungkan beberapa komponen penalti\n",
        "- Menyesuaikan perilaku loss terhadap domain tertentu\n",
        "\n",
        "Sebagai contoh, Huber Loss sering digunakan karena lebih stabil dibandingkan Mean Squared Error ketika data mengandung outlier.\n"
      ],
      "metadata": {
        "id": "fPMZ4mD6fjBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss  = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# Penggunaan dalam Model\n",
        "# model.compile(loss=huber_fn, optimizer=\"nadam\")"
      ],
      "metadata": {
        "id": "cx7sZ6GMf0tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Custom Layers dan Custom Models\n",
        "\n",
        "Keras memungkinkan pengembang untuk membuat layer kustom dengan melakukan subclassing terhadap `keras.layers.Layer`. Pendekatan ini memberikan fleksibilitas tinggi, terutama ketika:\n",
        "- Operasi yang dibutuhkan tidak tersedia dalam layer standar\n",
        "- Struktur perhitungan dalam layer bersifat tidak konvensional\n",
        "- Model perlu dioptimalkan secara spesifik\n",
        "\n",
        "Dengan mendefinisikan metode `build()` dan `call()`, kita dapat mengontrol pembuatan bobot serta alur komputasi forward pass secara eksplisit.\n"
      ],
      "metadata": {
        "id": "3VTMRwE2fj3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDense(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        # Membuat bobot (weights)\n",
        "        self.kernel = self.add_weight(name=\"kernel\",\n",
        "                                      shape=[batch_input_shape[-1], self.units],\n",
        "                                      initializer=\"glorot_normal\")\n",
        "        self.bias = self.add_weight(name=\"bias\", shape=[self.units],\n",
        "                                    initializer=\"zeros\")\n",
        "        super().build(batch_input_shape)\n",
        "\n",
        "    def call(self, X):\n",
        "        return self.activation(X @ self.kernel + self.bias)\n",
        "\n",
        "# Menggunakan layer kustom dalam model\n",
        "model = tf.keras.models.Sequential([\n",
        "    MyDense(30, activation=\"relu\", input_shape=[8]),\n",
        "    MyDense(1)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "EOAQb2KPf4Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Autodifferentiation dengan GradientTape\n",
        "\n",
        "Salah satu kekuatan utama TensorFlow adalah kemampuannya dalam menghitung turunan secara otomatis melalui mekanisme *automatic differentiation*.\n",
        "\n",
        "Objek `tf.GradientTape` mencatat seluruh operasi matematika yang terjadi selama eksekusi forward pass. Informasi ini kemudian digunakan untuk menghitung gradien terhadap variabel tertentu saat proses backpropagation.\n",
        "\n",
        "Pendekatan ini memungkinkan perhitungan gradien yang fleksibel, bahkan untuk fungsi matematika yang kompleks dan tidak terstruktur.\n"
      ],
      "metadata": {
        "id": "2D5Qh5-Pfky7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(w1, w2):\n",
        "    return 3 * w1**2 + 2 * w1 * w2\n",
        "\n",
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
        "with tf.GradientTape() as tape:\n",
        "    z = f(w1, w2)\n",
        "\n",
        "gradients = tape.gradient(z, [w1, w2])\n",
        "print(\"Gradients [dz/dw1, dz/dw2]:\", gradients)"
      ],
      "metadata": {
        "id": "dogXeth2f7T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Custom Training Loops\n",
        "\n",
        "Meskipun metode `.fit()` sangat praktis, terdapat situasi di mana kita memerlukan kendali penuh terhadap setiap langkah training. Dalam kondisi tersebut, custom training loop menjadi solusi utama.\n",
        "\n",
        "Dengan menulis training loop secara manual, kita dapat:\n",
        "- Mengatur update bobot secara eksplisit\n",
        "- Menggabungkan beberapa objective function\n",
        "- Mengimplementasikan algoritma training non-standar\n",
        "\n",
        "Pendekatan ini umum digunakan pada reinforcement learning, meta-learning, dan penelitian eksperimental.\n"
      ],
      "metadata": {
        "id": "efkax0rkflm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skenario sederhana Custom Training Loop\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "\n",
        "def train_step(model, X_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_batch)\n",
        "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "gQMs5lxGf_sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. TensorFlow Functions dan Computational Graph\n",
        "\n",
        "Secara default, TensorFlow berjalan dalam *eager execution mode*, yang memudahkan debugging namun kurang optimal dari sisi performa.\n",
        "\n",
        "Decorator `@tf.function` memungkinkan konversi fungsi Python menjadi computational graph TensorFlow. Dengan pendekatan ini:\n",
        "- Overhead Python dapat diminimalkan\n",
        "- Eksekusi menjadi lebih cepat\n",
        "- Kode lebih siap untuk deployment produksi\n",
        "\n",
        "Namun, penggunaan `@tf.function` memerlukan perhatian ekstra karena tidak semua operasi Python dapat dikonversi menjadi graph.\n"
      ],
      "metadata": {
        "id": "vJv2-2eofmmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def tf_cube(x):\n",
        "    return x ** 3\n",
        "\n",
        "print(\"Cube of 2:\", tf_cube(tf.constant(2.0)))"
      ],
      "metadata": {
        "id": "J5AcxAkUgCST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kesimpulan Chapter 12\n",
        "\n",
        "Chapter ini menekankan pentingnya memahami TensorFlow pada level yang lebih mendalam. Dengan menguasai custom components, autodifferentiation, dan training loop manual, pengembang tidak lagi terbatas pada API siap pakai.\n",
        "\n",
        "Kemampuan ini sangat penting bagi praktisi dan peneliti yang ingin:\n",
        "- Mengimplementasikan arsitektur neural network baru\n",
        "- Mengadaptasi algoritma dari paper penelitian\n",
        "- Mengoptimalkan performa model untuk kasus khusus\n",
        "\n",
        "Dengan fondasi ini, kita siap melangkah ke pengembangan deep learning tingkat lanjut.\n"
      ],
      "metadata": {
        "id": "VxDC5Zoifn5c"
      }
    }
  ]
}