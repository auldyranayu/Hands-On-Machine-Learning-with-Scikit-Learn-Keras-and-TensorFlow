{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 9: Unsupervised Learning Techniques\n",
        "\n",
        "Chapter ini membahas **Unsupervised Learning**, yaitu pendekatan Machine Learning yang bekerja tanpa label target. Berbeda dengan supervised learning yang membutuhkan pasangan data (X, y), pada unsupervised learning model hanya menerima data input dan diminta untuk menemukan struktur, pola, atau keteraturan yang tersembunyi di dalamnya.\n",
        "\n",
        "Dalam praktik nyata, sebagian besar data yang tersedia tidak memiliki label. Proses pelabelan sering kali mahal, memakan waktu, dan membutuhkan keahlian khusus. Oleh karena itu, unsupervised learning memainkan peran fundamental dalam analisis data modern.\n",
        "\n",
        "Pendekatan ini banyak digunakan untuk eksplorasi data, preprocessing, serta sebagai fondasi bagi metode Machine Learning yang lebih kompleks.\n"
      ],
      "metadata": {
        "id": "eakgATsZI2oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pengantar Unsupervised Learning\n",
        "\n",
        "Unsupervised learning bertujuan untuk mengekstraksi informasi bermakna dari data tanpa supervisi eksplisit. Model tidak mengetahui jawaban yang benar, melainkan belajar langsung dari pola alami yang muncul pada data.\n",
        "\n",
        "Contoh data yang umum dianalisis dengan unsupervised learning meliputi:\n",
        "- data perilaku pengguna,\n",
        "- data transaksi pelanggan,\n",
        "- data sensor industri,\n",
        "- kumpulan dokumen teks,\n",
        "- data citra dan audio berskala besar.\n",
        "\n",
        "Pendekatan ini memungkinkan sistem memahami struktur data secara mandiri dan sering digunakan sebagai tahap awal sebelum model supervised learning.\n"
      ],
      "metadata": {
        "id": "ykHBKAc1JPZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tugas Utama dalam Unsupervised Learning\n",
        "\n",
        "Unsupervised learning mencakup beberapa tugas utama, antara lain:\n",
        "\n",
        "- **Clustering**, yaitu pengelompokan data berdasarkan tingkat kemiripan.\n",
        "- **Anomaly Detection**, yaitu identifikasi data yang menyimpang dari pola normal.\n",
        "- **Density Estimation**, yaitu pemodelan distribusi probabilitas data.\n",
        "\n",
        "Masing-masing tugas memiliki aplikasi luas dalam bidang bisnis, keamanan, kesehatan, dan analisis data eksploratif.\n"
      ],
      "metadata": {
        "id": "d_sGbBz7JSFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Hubungan dengan Chapter Sebelumnya\n",
        "\n",
        "Pada Chapter 8 telah dibahas teknik **Dimensionality Reduction**, yang juga termasuk kategori unsupervised learning. Teknik tersebut berfokus pada penyederhanaan data berdimensi tinggi.\n",
        "\n",
        "Chapter 9 memperluas konsep tersebut dengan membahas teknik unsupervised learning yang berfokus pada pengelompokan data, deteksi penyimpangan, dan pemahaman struktur distribusi data.\n"
      ],
      "metadata": {
        "id": "q0dmRWnAJTEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Clustering\n",
        "\n",
        "Clustering merupakan salah satu tugas utama dalam unsupervised learning yang bertujuan membagi data ke dalam beberapa kelompok (cluster) berdasarkan kemiripan.\n",
        "\n",
        "Tidak seperti klasifikasi:\n",
        "- tidak ada label awal,\n",
        "- jumlah cluster sering kali tidak diketahui,\n",
        "- evaluasi hasil bersifat kontekstual dan eksploratif.\n"
      ],
      "metadata": {
        "id": "HHzlVmvyJUi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Intuisi Clustering\n",
        "\n",
        "Intuisi dasar clustering adalah bahwa data dalam satu cluster memiliki kemiripan yang lebih tinggi dibandingkan dengan data di cluster lain.\n",
        "\n",
        "Kemiripan biasanya diukur menggunakan metrik jarak seperti Euclidean distance, meskipun metrik lain dapat digunakan sesuai dengan karakteristik data.\n"
      ],
      "metadata": {
        "id": "cM3vbHjIJVtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Use Cases Clustering\n",
        "\n",
        "Clustering digunakan secara luas dalam berbagai aplikasi, antara lain:\n",
        "- segmentasi pelanggan,\n",
        "- pengelompokan dokumen,\n",
        "- analisis citra,\n",
        "- bioinformatika,\n",
        "- eksplorasi data awal.\n",
        "\n",
        "Sering kali clustering digunakan bukan sebagai tujuan akhir, melainkan sebagai alat bantu analisis.\n"
      ],
      "metadata": {
        "id": "jkrdvaqIJXHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Tantangan dalam Clustering\n",
        "\n",
        "Beberapa tantangan utama dalam clustering meliputi:\n",
        "- pemilihan jumlah cluster yang tepat,\n",
        "- sensitivitas terhadap noise dan outlier,\n",
        "- kesulitan evaluasi tanpa label,\n",
        "- performa menurun pada data berdimensi tinggi.\n"
      ],
      "metadata": {
        "id": "79Vqttv_JYEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. K-Means Clustering\n",
        "\n",
        "K-Means merupakan algoritma clustering yang paling populer karena sederhana dan efisien. Tujuan K-Means adalah meminimalkan jarak antara setiap data dan pusat cluster (centroid) terdekatnya.\n"
      ],
      "metadata": {
        "id": "G_0jNq1PJY4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Intuisi Dasar K-Means\n",
        "\n",
        "Algoritma K-Means bekerja secara iteratif melalui langkah berikut:\n",
        "1. Menentukan jumlah cluster K.\n",
        "2. Menginisialisasi centroid awal.\n",
        "3. Menetapkan setiap data ke centroid terdekat.\n",
        "4. Memperbarui centroid sebagai rata-rata data dalam cluster.\n",
        "5. Mengulangi proses hingga konvergen.\n"
      ],
      "metadata": {
        "id": "TQPA29tPJZuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Fungsi Objektif K-Means\n",
        "\n",
        "Secara matematis, K-Means meminimalkan total jarak kuadrat antara setiap titik data dan centroid cluster-nya. Nilai ini dikenal sebagai **inertia**.\n"
      ],
      "metadata": {
        "id": "dkWJWpGgJah4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X_blobs, y_blobs = make_blobs(\n",
        "    n_samples=300,\n",
        "    centers=4,\n",
        "    cluster_std=0.60,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X_blobs)\n",
        "\n",
        "kmeans.cluster_centers_\n"
      ],
      "metadata": {
        "id": "We-dd06uJbsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 K-Means dengan Scikit-Learn\n",
        "\n",
        "Setelah model dilatih, setiap data akan memiliki label cluster yang menunjukkan keanggotaan cluster masing-masing.\n"
      ],
      "metadata": {
        "id": "INUEDwZ-Jchh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans.fit(X_blobs)\n"
      ],
      "metadata": {
        "id": "55YPGgyfJeEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah training, kita dapat mengakses posisi centroid dan label cluster hasil K-Means."
      ],
      "metadata": {
        "id": "fHf2IvkYKCQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "KSZsUcGjKEF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_[:10]"
      ],
      "metadata": {
        "id": "39lF8mUQKFyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluating K-Means: Inertia\n",
        "\n",
        "Inertia mengukur seberapa rapat cluster yang dihasilkan. Nilai inertia yang kecil menunjukkan cluster yang lebih kompak, tetapi inertia selalu menurun ketika jumlah cluster bertambah.\n"
      ],
      "metadata": {
        "id": "-2KvLB3OJdVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 The Elbow Method\n",
        "\n",
        "Elbow Method digunakan untuk menentukan jumlah cluster optimal dengan mencari titik di mana penurunan inertia mulai melambat.\n"
      ],
      "metadata": {
        "id": "oB8_QnOcJiMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inertias = []\n",
        "\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_blobs)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "inertias\n"
      ],
      "metadata": {
        "id": "1X4449AXJjDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Initialization of Centroids\n",
        "\n",
        "Pemilihan centroid awal sangat memengaruhi hasil K-Means. Inisialisasi yang buruk dapat menyebabkan solusi lokal yang tidak optimal.\n"
      ],
      "metadata": {
        "id": "sjspJJYsJlKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 K-Means++\n",
        "\n",
        "K-Means++ merupakan metode inisialisasi centroid yang memilih titik awal secara strategis agar tersebar dengan baik. Metode ini digunakan secara default pada Scikit-Learn.\n"
      ],
      "metadata": {
        "id": "teO3Oa6EJmcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Limitations of K-Means\n",
        "\n",
        "Keterbatasan K-Means meliputi:\n",
        "- harus menentukan jumlah cluster di awal,\n",
        "- sensitif terhadap outlier,\n",
        "- tidak cocok untuk cluster non-bulat,\n",
        "- tidak mempertimbangkan perbedaan densitas.\n"
      ],
      "metadata": {
        "id": "_0ba8n1SJnit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. K-Means for Image Segmentation\n",
        "\n",
        "K-Means dapat digunakan untuk segmentasi citra dengan mengelompokkan piksel berdasarkan kemiripan warna.\n"
      ],
      "metadata": {
        "id": "4OTvrkUFJpCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Selain digunakan untuk data numerik umum, algoritma K-Means juga dapat dimanfaatkan dalam bidang pengolahan citra, khususnya untuk **image segmentation**. Pada konteks ini, setiap piksel dianggap sebagai sebuah titik data, biasanya direpresentasikan oleh nilai warna (RGB).\n",
        "\n",
        "Tujuan image segmentation adalah membagi citra menjadi beberapa wilayah homogen sehingga objek atau pola tertentu dapat lebih mudah dianalisis.\n"
      ],
      "metadata": {
        "id": "h-V0SYCNLGO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Intuisi Image Segmentation dengan K-Means\n",
        "\n",
        "Intuisi dasar segmentasi citra menggunakan K-Means adalah mengelompokkan piksel yang memiliki kemiripan warna ke dalam satu cluster yang sama. Setiap cluster merepresentasikan satu segmen citra.\n",
        "\n",
        "Dengan cara ini, jumlah warna pada citra dapat direduksi secara signifikan tanpa menghilangkan struktur visual utama.\n"
      ],
      "metadata": {
        "id": "g4dXWMqvLKH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Contoh Implementasi Konseptual\n",
        "\n",
        "Secara konseptual, proses segmentasi citra dengan K-Means dilakukan melalui langkah berikut:\n",
        "1. Mengubah citra menjadi matriks dua dimensi (piksel Ã— fitur warna).\n",
        "2. Menentukan jumlah cluster K.\n",
        "3. Melatih K-Means menggunakan data piksel.\n",
        "4. Mengganti warna setiap piksel dengan warna centroid cluster-nya.\n"
      ],
      "metadata": {
        "id": "sbJXyZKWLLN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# contoh data piksel (RGB) sederhana\n",
        "pixels = np.array([\n",
        "    [255, 0, 0],\n",
        "    [254, 0, 0],\n",
        "    [0, 255, 0],\n",
        "    [0, 254, 0],\n",
        "    [0, 0, 255],\n",
        "    [0, 0, 254]\n",
        "])\n",
        "\n",
        "kmeans_img = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_img.fit(pixels)\n",
        "\n",
        "kmeans_img.cluster_centers_\n",
        "\n"
      ],
      "metadata": {
        "id": "Ym06JakhQ8fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Kelebihan dan Kekurangan\n",
        "\n",
        "**Kelebihan:**\n",
        "- Implementasi sederhana\n",
        "- Efisien untuk citra berukuran sedang\n",
        "- Mengurangi kompleksitas warna\n",
        "\n",
        "**Kekurangan:**\n",
        "- Harus menentukan jumlah cluster\n",
        "- Sensitif terhadap noise\n",
        "- Tidak mempertimbangkan hubungan spasial antar piksel\n"
      ],
      "metadata": {
        "id": "NLTb8clDLNkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pixels = np.array([\n",
        "    [255, 0, 0],\n",
        "    [254, 0, 0],\n",
        "    [0, 255, 0],\n",
        "    [0, 254, 0],\n",
        "    [0, 0, 255],\n",
        "    [0, 0, 254]\n",
        "])\n",
        "\n",
        "kmeans_img = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_img.fit(pixels)\n",
        "\n",
        "kmeans_img.cluster_centers_\n"
      ],
      "metadata": {
        "id": "4hl9COAZJqQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Gaussian Mixture Models (GMM)\n",
        "\n",
        "Gaussian Mixture Models merupakan pendekatan clustering probabilistik yang mengasumsikan bahwa data dihasilkan dari kombinasi beberapa distribusi Gaussian.\n",
        "\n",
        "Berbeda dengan K-Means yang menggunakan assignment keras (hard assignment), GMM memberikan probabilitas keanggotaan untuk setiap cluster.\n"
      ],
      "metadata": {
        "id": "51j-r_C1LPLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Intuisi Gaussian Mixture Models\n",
        "\n",
        "Pada GMM, setiap cluster direpresentasikan oleh sebuah distribusi Gaussian dengan parameter mean dan covariance tertentu. Setiap titik data memiliki probabilitas untuk berasal dari masing-masing distribusi tersebut.\n"
      ],
      "metadata": {
        "id": "fOxsihooLRWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Expectation-Maximization (EM) Algorithm\n",
        "\n",
        "GMM dilatih menggunakan algoritma Expectation-Maximization (EM), yang bekerja secara iteratif:\n",
        "- **E-step**: Menghitung probabilitas keanggotaan cluster.\n",
        "- **M-step**: Memperbarui parameter distribusi Gaussian.\n"
      ],
      "metadata": {
        "id": "U-V-BU61LSQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 GMM dengan Scikit-Learn\n",
        "\n",
        "Setelah model GMM dilatih, setiap data dapat diberi label berdasarkan probabilitas tertinggi.\n"
      ],
      "metadata": {
        "id": "88LIYyumLUys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=4, random_state=42)\n",
        "gmm.fit(X_blobs)\n",
        "\n",
        "gmm.means_\n",
        ""
      ],
      "metadata": {
        "id": "eLgqlUoFLV-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean dari masing-masing Gaussian merepresentasikan pusat cluster hasil GMM."
      ],
      "metadata": {
        "id": "sUZLz1y_RGLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmm.predict(X_blobs[:10])"
      ],
      "metadata": {
        "id": "qmrqzAGbRHe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediksi GMM memberikan label cluster berdasarkan Gaussian dengan probabilitas tertinggi."
      ],
      "metadata": {
        "id": "G_DvDjtRRLPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Namun, kita juga dapat mengakses probabilitas keanggotaan setiap cluster."
      ],
      "metadata": {
        "id": "6C1dM53bRMm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmm.predict_proba(X_blobs[:5])"
      ],
      "metadata": {
        "id": "JvrQUKOpRNtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Soft assignment ini membuat GMM lebih ekspresif dibandingkan K-Means, terutama ketika cluster saling tumpang tindih."
      ],
      "metadata": {
        "id": "7RBff3N7RQeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Selecting the Number of Clusters\n",
        "\n",
        "Menentukan jumlah cluster yang optimal merupakan tantangan penting dalam clustering probabilistik. GMM menyediakan kriteria statistik untuk membantu pemilihan model.\n"
      ],
      "metadata": {
        "id": "OwpAWINuLW2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Akaike Information Criterion (AIC)\n",
        "\n",
        "AIC mengukur kualitas model dengan mempertimbangkan trade-off antara goodness-of-fit dan kompleksitas model.\n"
      ],
      "metadata": {
        "id": "Oifbp89HLYIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Bayesian Information Criterion (BIC)\n",
        "\n",
        "BIC mirip dengan AIC tetapi memberikan penalti yang lebih besar terhadap model yang kompleks.\n"
      ],
      "metadata": {
        "id": "0ETz8hrjLZQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Menggunakan AIC dan BIC pada GMM\n",
        "\n",
        "Model terbaik biasanya ditentukan berdasarkan nilai AIC atau BIC terendah.\n"
      ],
      "metadata": {
        "id": "fNQK6ooOLcNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn menyediakan metode aic() dan bic() untuk mengevaluasi model GMM dengan jumlah komponen yang berbeda."
      ],
      "metadata": {
        "id": "7wRKJdN8RTpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aics = []\n",
        "bics = []\n",
        "\n",
        "for k in range(1, 10):\n",
        "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "    gmm.fit(X_blobs)\n",
        "    aics.append(gmm.aic(X_blobs))\n",
        "    bics.append(gmm.bic(X_blobs))\n",
        "\n",
        "aics, bics\n",
        "\n"
      ],
      "metadata": {
        "id": "7_zpYJwDRVjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan memplot nilai AIC dan BIC terhadap jumlah komponen, kita dapat memilih jumlah cluster yang meminimalkan kriteria tersebut."
      ],
      "metadata": {
        "id": "MGFDlM0CRX_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Anomaly Detection\n",
        "\n",
        "Anomaly detection bertujuan mengidentifikasi data yang menyimpang secara signifikan dari pola umum.\n"
      ],
      "metadata": {
        "id": "YHXQucfyLdz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.1 Tantangan dalam Anomaly Detection\n",
        "\n",
        "Tantangan utama meliputi:\n",
        "- ketidakseimbangan data,\n",
        "- definisi anomali yang ambigu,\n",
        "- perubahan distribusi data.\n"
      ],
      "metadata": {
        "id": "QKRxv2wTLh7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "iso_forest = IsolationForest(random_state=42)\n",
        "iso_forest.fit(X_blobs)\n",
        "\n",
        "iso_forest.predict(X_blobs)[:10]\n"
      ],
      "metadata": {
        "id": "EDN9Xln7LjXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2 Isolation Forest\n",
        "\n",
        "Isolation Forest mendeteksi anomali dengan mengisolasi data melalui struktur pohon acak.\n"
      ],
      "metadata": {
        "id": "VVzrntiDLnwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.3 Local Outlier Factor (LOF)\n",
        "\n",
        "LOF mengukur kepadatan lokal suatu titik dibandingkan dengan tetangganya.\n"
      ],
      "metadata": {
        "id": "isS1tSwhLo_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor()\n",
        "lof.fit_predict(X_blobs)[:10]\n"
      ],
      "metadata": {
        "id": "aubCrCbcLqLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Novelty Detection\n",
        "\n",
        "Novelty Detection merupakan pendekatan dalam unsupervised learning yang bertujuan untuk mendeteksi data baru yang berbeda secara signifikan dari data yang dianggap normal.\n",
        "\n",
        "Berbeda dengan anomaly detection, novelty detection diasumsikan **dilatih hanya menggunakan data normal**, tanpa mengandung outlier selama proses training.\n",
        "\n",
        "Pendekatan ini banyak digunakan pada sistem monitoring dan quality control, di mana model dilatih pada kondisi normal dan digunakan untuk mendeteksi penyimpangan di masa depan.\n"
      ],
      "metadata": {
        "id": "puN1xjprQqE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13.1 One-Class Support Vector Machine (One-Class SVM)\n",
        "\n",
        "One-Class Support Vector Machine merupakan algoritma populer untuk melakukan novelty detection.\n",
        "\n",
        "Tujuan utama One-Class SVM adalah mempelajari batas (decision boundary) yang mencakup sebagian besar data normal, kemudian mengklasifikasikan data baru yang berada di luar batas tersebut sebagai novelty.\n",
        "\n",
        "Secara intuitif:\n",
        "- Data normal membentuk satu kelompok besar\n",
        "- Model mencari hyperplane yang memisahkan data tersebut dari titik asal\n",
        "- Data yang berada jauh dari distribusi normal dianggap sebagai data baru atau tidak wajar\n",
        "\n",
        "Pendekatan ini mirip dengan Support Vector Machine klasik, tetapi tanpa kelas negatif eksplisit.\n"
      ],
      "metadata": {
        "id": "7-85Up78Qrk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13.2 One-Class SVM dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi One-Class SVM melalui class `OneClassSVM`.\n",
        "\n",
        "Model ini menggunakan kernel (sering kali RBF) untuk menangkap pola non-linear pada data normal.\n"
      ],
      "metadata": {
        "id": "lfSCSaXRQsqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "\n",
        "one_class_svm = OneClassSVM(kernel=\"rbf\", gamma=0.1, nu=0.05)\n",
        "one_class_svm.fit(X_blobs)\n",
        "\n",
        "one_class_svm.predict(X_blobs[:10])\n"
      ],
      "metadata": {
        "id": "n8EJR4M6Qui9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output model berupa:\n",
        "- Nilai **1** untuk data yang dianggap normal\n",
        "- Nilai **-1** untuk data yang dianggap sebagai novelty\n",
        "\n",
        "Parameter penting:\n",
        "- `nu` mengontrol perkiraan proporsi data abnormal\n",
        "- `gamma` menentukan kompleksitas boundary model\n",
        "\n",
        "Pemilihan parameter yang tidak tepat dapat menyebabkan overfitting atau underfitting.\n"
      ],
      "metadata": {
        "id": "97dj1zYWQw9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13.3 Perbedaan Novelty Detection dan Anomaly Detection\n",
        "\n",
        "Meskipun sering disamakan, novelty detection dan anomaly detection memiliki perbedaan mendasar.\n",
        "\n",
        "**Anomaly Detection:**\n",
        "- Data training dapat mengandung outlier\n",
        "- Fokus pada mendeteksi penyimpangan dalam dataset yang ada\n",
        "- Contoh: deteksi penipuan transaksi\n",
        "\n",
        "**Novelty Detection:**\n",
        "- Data training diasumsikan bersih (tanpa outlier)\n",
        "- Fokus pada mendeteksi data baru yang berbeda\n",
        "- Contoh: sistem quality control manufaktur\n",
        "\n",
        "Perbedaan asumsi ini sangat penting dalam pemilihan metode yang tepat.\n"
      ],
      "metadata": {
        "id": "EpJ8EKnkQyOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Density Estimation\n",
        "\n",
        "Density estimation merupakan salah satu tugas penting dalam unsupervised learning yang bertujuan untuk memperkirakan distribusi probabilitas dari data.\n",
        "\n",
        "Dengan memahami distribusi data, kita dapat:\n",
        "- mendeteksi anomali,\n",
        "- menghasilkan data sintetis,\n",
        "- memahami pola global data,\n",
        "- serta melakukan analisis probabilistik.\n",
        "\n",
        "Pendekatan ini sering digunakan sebagai dasar anomaly detection dan novelty detection.\n"
      ],
      "metadata": {
        "id": "YRHDUHicPG-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14.1 Kernel Density Estimation (KDE)\n",
        "\n",
        "Kernel Density Estimation (KDE) adalah metode non-parametrik untuk memperkirakan fungsi densitas probabilitas dari data.\n",
        "\n",
        "Berbeda dengan Gaussian Mixture Models (GMM) yang mengasumsikan bentuk distribusi tertentu, KDE tidak mengasumsikan bentuk distribusi data.\n",
        "\n",
        "Intuisi KDE adalah:\n",
        "- setiap titik data dianggap sebagai pusat kernel,\n",
        "- kernel yang umum digunakan adalah Gaussian,\n",
        "- densitas total merupakan penjumlahan kontribusi semua kernel.\n",
        "\n",
        "Pendekatan ini menghasilkan estimasi distribusi yang lebih halus dan fleksibel.\n"
      ],
      "metadata": {
        "id": "t9tDnp9dPION"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14.2 Parameter Bandwidth\n",
        "\n",
        "Parameter terpenting dalam KDE adalah bandwidth, yang menentukan lebar kernel.\n",
        "\n",
        "Bandwidth sangat memengaruhi hasil estimasi densitas:\n",
        "- bandwidth kecil menghasilkan distribusi yang sangat detail (berisiko overfitting),\n",
        "- bandwidth besar menghasilkan distribusi yang terlalu halus (berisiko underfitting).\n",
        "\n",
        "Pemilihan bandwidth yang tepat sangat krusial untuk menghasilkan estimasi distribusi yang akurat dan bermakna.\n",
        "\n",
        "Dalam praktik, bandwidth sering ditentukan melalui:\n",
        "- eksperimen manual,\n",
        "- cross-validation,\n",
        "- atau aturan heuristik tertentu.\n"
      ],
      "metadata": {
        "id": "J8DpvwE4PKMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14.3 KDE dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi Kernel Density Estimation melalui class KernelDensity.\n",
        "\n",
        "Class ini memungkinkan kita memilih jenis kernel dan nilai bandwidth sesuai kebutuhan data.\n",
        "\n",
        "Metode score_samples() digunakan untuk menghitung log-density dari setiap data point.\n"
      ],
      "metadata": {
        "id": "38lqukFgPLUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.5)\n",
        "kde.fit(X_blobs)\n",
        "\n",
        "log_density = kde.score_samples(X_blobs[:10])\n",
        "log_density\n"
      ],
      "metadata": {
        "id": "4lcGZbIbPMfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nilai yang dihasilkan merupakan log-density, yaitu logaritma dari estimasi densitas probabilitas.\n",
        "\n",
        "Interpretasi hasil:\n",
        "- nilai log-density yang sangat kecil (negatif besar) menunjukkan data berada di area jarang,\n",
        "- nilai ini sering digunakan sebagai indikator anomali,\n",
        "- semakin rendah densitas, semakin besar kemungkinan data tersebut menyimpang dari pola umum.\n",
        "\n",
        "Karena sifatnya yang fleksibel, KDE sering digunakan sebagai alat eksplorasi dan anomaly detection berbasis probabilitas.\n"
      ],
      "metadata": {
        "id": "ZqI4eJ4WPNha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. Semi-Supervised Learning\n",
        "\n",
        "Semi-supervised learning merupakan pendekatan pembelajaran mesin yang berada di antara supervised learning dan unsupervised learning.\n",
        "\n",
        "Pendekatan ini digunakan ketika hanya sebagian kecil data yang memiliki label, sementara sebagian besar data lainnya tidak berlabel.\n",
        "\n",
        "Kondisi ini sangat umum di dunia nyata karena proses pelabelan data sering kali:\n",
        "- mahal secara biaya,\n",
        "- memakan waktu,\n",
        "- membutuhkan keahlian khusus.\n",
        "\n",
        "Semi-supervised learning bertujuan untuk memanfaatkan struktur alami data tidak berlabel agar proses pembelajaran menjadi lebih efektif dibandingkan hanya mengandalkan data berlabel yang terbatas.\n"
      ],
      "metadata": {
        "id": "nOFpEiRPR95X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15.1 Intuisi Semi-Supervised Learning\n",
        "\n",
        "Intuisi utama semi-supervised learning didasarkan pada asumsi bahwa data tidak berlabel mengandung informasi struktural yang sangat berharga.\n",
        "\n",
        "Beberapa asumsi penting dalam semi-supervised learning antara lain:\n",
        "- Data yang saling berdekatan cenderung memiliki label yang sama\n",
        "- Data membentuk cluster alami di ruang fitur\n",
        "- Batas keputusan sebaiknya melewati area dengan kepadatan data rendah\n",
        "\n",
        "Dengan memanfaatkan asumsi tersebut, model dapat:\n",
        "- mempelajari pola dari data berlabel,\n",
        "- memperluas informasi tersebut ke data tidak berlabel,\n",
        "- meningkatkan performa prediksi secara keseluruhan.\n",
        "\n",
        "Dalam banyak kasus, semi-supervised learning menghasilkan performa yang jauh lebih baik dibandingkan supervised learning murni dengan jumlah label yang sangat terbatas.\n"
      ],
      "metadata": {
        "id": "2HvTRO7oSBCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15.2 Label Propagation\n",
        "\n",
        "Label Propagation merupakan salah satu algoritma semi-supervised learning yang paling populer.\n",
        "\n",
        "Algoritma ini bekerja dengan menyebarkan label dari data berlabel ke data tidak berlabel berdasarkan kemiripan antar data.\n",
        "\n",
        "Konsep utama Label Propagation adalah:\n",
        "- membangun graf kemiripan antar data,\n",
        "- mengasumsikan bahwa node yang saling terhubung kuat memiliki label yang sama,\n",
        "- menyebarkan label melalui graf hingga mencapai kondisi stabil.\n",
        "\n",
        "Proses ini bersifat iteratif dan berhenti ketika label tidak lagi berubah secara signifikan.\n",
        "\n",
        "Label Propagation sangat efektif ketika:\n",
        "- data membentuk cluster yang jelas,\n",
        "- hubungan antar data dapat direpresentasikan dengan baik menggunakan graf,\n",
        "- jumlah data tidak berlabel jauh lebih banyak dibandingkan data berlabel.\n"
      ],
      "metadata": {
        "id": "9g07B4T2SCMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15.3 Label Propagation dengan Scikit-Learn\n",
        "\n",
        "Scikit-Learn menyediakan implementasi Label Propagation melalui class `LabelPropagation` dan `LabelSpreading`.\n",
        "\n",
        "Pada contoh berikut, sebagian label data sengaja dihilangkan untuk mensimulasikan skenario semi-supervised learning.\n"
      ],
      "metadata": {
        "id": "5xyiLT4-SDYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "import numpy as np\n",
        "\n",
        "# mensimulasikan data semi-supervised\n",
        "y_semi = np.copy(y_blobs)\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "unlabeled_indices = rng.choice(len(y_semi), size=200, replace=False)\n",
        "y_semi[unlabeled_indices] = -1\n",
        "\n",
        "label_prop = LabelPropagation()\n",
        "label_prop.fit(X_blobs, y_semi)\n",
        "\n",
        "label_prop.transduction_[:10]\n"
      ],
      "metadata": {
        "id": "vYqPfaeMSEdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Summary (Chapter 9)\n",
        "\n",
        "Chapter ini membahas konsep dan teknik utama dalam unsupervised learning, termasuk clustering dengan K-Means, evaluasi cluster, serta penerapannya dalam berbagai konteks.\n",
        "\n",
        "Unsupervised learning merupakan fondasi penting dalam analisis data modern karena memungkinkan pemahaman struktur data tanpa ketergantungan pada label.\n"
      ],
      "metadata": {
        "id": "c4d56bSMJreM"
      }
    }
  ]
}