# ============================================================
# Chapter 2: End-to-End Machine Learning Project
# ============================================================

# ------------------------------------------------------------
# 1. Look at the Big Picture
# ------------------------------------------------------------
# Pada bagian ini, kita memahami konteks proyek machine learning
# secara keseluruhan. Kita berpura-pura sebagai data scientist
# yang bekerja di perusahaan real estate dan diminta memprediksi
# median harga rumah di California berdasarkan data sensus.
#
# Fokus utama bukan algoritma, tetapi alur kerja end-to-end.

# ------------------------------------------------------------
# 1.1 Framing the Problem
# ------------------------------------------------------------
# Masalah ini adalah supervised learning karena memiliki label.
# Target berupa nilai kontinu, sehingga ini adalah regression task.
# Model akan dilatih secara batch karena data relatif statis.

# ------------------------------------------------------------
# 1.2 Select a Performance Measure
# ------------------------------------------------------------
# Metrik yang digunakan adalah RMSE (Root Mean Squared Error)
# karena penalti error besar lebih tinggi dan sesuai konteks harga.

# ------------------------------------------------------------
# 2. Get the Data
# ------------------------------------------------------------
# Dataset yang digunakan adalah California Housing Dataset.

from sklearn.datasets import fetch_california_housing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

housing = fetch_california_housing(as_frame=True)
df = housing.frame

# ------------------------------------------------------------
# 2.1 Create the Workspace
# ------------------------------------------------------------
# Workspace disiapkan dengan mengimpor library utama dan
# memastikan data dapat diproses ulang kapan saja.

# ------------------------------------------------------------
# 2.2 Downloading the Data
# ------------------------------------------------------------
# Dataset diambil langsung dari sklearn, tidak perlu download manual.

# ------------------------------------------------------------
# 2.3 Loading the Data
# ------------------------------------------------------------
# Data dimuat ke dalam DataFrame pandas untuk kemudahan analisis.

df.head()

# ------------------------------------------------------------
# 3. Take a Quick Look at the Data Structure
# ------------------------------------------------------------
# Tahap ini bertujuan memahami struktur data secara cepat.

# ------------------------------------------------------------
# 3.1 Data Overview with info()
# ------------------------------------------------------------
df.info()

# ------------------------------------------------------------
# 3.2 Descriptive Statistics
# ------------------------------------------------------------
df.describe()

# ------------------------------------------------------------
# 3.3 Categorical Attribute: ocean_proximity
# ------------------------------------------------------------
# Dataset sklearn tidak memiliki ocean_proximity secara default.
# Oleh karena itu kita membuat fitur kategorikal buatan.

df["ocean_proximity"] = np.random.choice(
    ["<1H OCEAN", "INLAND", "NEAR OCEAN"], size=len(df)
)

df["ocean_proximity"].value_counts()

# ------------------------------------------------------------
# 4. Create a Test Set
# ------------------------------------------------------------
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(
    df, test_size=0.2, random_state=42
)

# ------------------------------------------------------------
# 4.1 Random Sampling
# ------------------------------------------------------------
# Random sampling membagi data secara acak.

# ------------------------------------------------------------
# 4.2 Stratified Sampling
# ------------------------------------------------------------
df["income_cat"] = pd.cut(
    df["MedInc"],
    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
    labels=[1, 2, 3, 4, 5]
)

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in split.split(df, df["income_cat"]):
    strat_train_set = df.loc[train_idx]
    strat_test_set = df.loc[test_idx]

# ------------------------------------------------------------
# 5. Discover and Visualize the Data
# ------------------------------------------------------------
housing = strat_train_set.copy()

# ------------------------------------------------------------
# 5.1 Visualizing Geographical Data
# ------------------------------------------------------------
housing.plot(kind="scatter", x="Longitude", y="Latitude", alpha=0.1)

# ------------------------------------------------------------
# 5.2 Visualizing Housing Prices
# ------------------------------------------------------------
housing.plot(
    kind="scatter",
    x="Longitude",
    y="Latitude",
    c="MedHouseVal",
    cmap="jet",
    alpha=0.4
)
plt.colorbar()

# ------------------------------------------------------------
# 5.3 Looking for Correlations
# ------------------------------------------------------------
corr_matrix = housing.corr()
corr_matrix["MedHouseVal"].sort_values(ascending=False)

# ------------------------------------------------------------
# 5.4 Scatter Matrix
# ------------------------------------------------------------
from pandas.plotting import scatter_matrix

scatter_matrix(
    housing[["MedHouseVal", "MedInc", "AveRooms"]],
    figsize=(10, 8)
)

# ------------------------------------------------------------
# 6. Experimenting with Attribute Combinations
# ------------------------------------------------------------
housing["rooms_per_household"] = housing["AveRooms"]
housing["population_per_household"] = housing["Population"] / housing["HouseAge"]

# ------------------------------------------------------------
# 6.2 Checking Correlation with New Attributes
# ------------------------------------------------------------
housing.corr()["MedHouseVal"].sort_values(ascending=False)

# ------------------------------------------------------------
# 7. Prepare the Data for Machine Learning Algorithms
# ------------------------------------------------------------
housing = strat_train_set.drop("MedHouseVal", axis=1)
housing_labels = strat_train_set["MedHouseVal"].copy()

# ------------------------------------------------------------
# 7.2 Handling Missing Values
# ------------------------------------------------------------
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
housing_num = housing.drop("ocean_proximity", axis=1)
imputer.fit(housing_num)
housing_num_tr = imputer.transform(housing_num)

# ------------------------------------------------------------
# 8. Handling Categorical Attributes
# ------------------------------------------------------------
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat = housing[["ocean_proximity"]]
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)

# ------------------------------------------------------------
# 11. Full Pipeline for Data Preparation
# ------------------------------------------------------------
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, housing_num.columns),
    ("cat", OneHotEncoder(), ["ocean_proximity"])
])

housing_prepared = full_pipeline.fit_transform(housing)

# ------------------------------------------------------------
# 12. Select and Train a Model
# ------------------------------------------------------------
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

# ------------------------------------------------------------
# 12.2 Evaluating the Model
# ------------------------------------------------------------
from sklearn.metrics import mean_squared_error

predictions = lin_reg.predict(housing_prepared)
rmse = np.sqrt(mean_squared_error(housing_labels, predictions))
rmse

# ------------------------------------------------------------
# 14. Random Forest Regressor
# ------------------------------------------------------------
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(random_state=42)
forest_reg.fit(housing_prepared, housing_labels)

# ------------------------------------------------------------
# 17. Feature Importance
# ------------------------------------------------------------
forest_reg.feature_importances_

# ------------------------------------------------------------
# Closing Summary (Chapter 2)
# ------------------------------------------------------------
# Chapter ini menunjukkan alur kerja Machine Learning end-to-end,
# dari memahami masalah, eksplorasi data, preprocessing, training,
# evaluasi, hingga interpretasi model.


