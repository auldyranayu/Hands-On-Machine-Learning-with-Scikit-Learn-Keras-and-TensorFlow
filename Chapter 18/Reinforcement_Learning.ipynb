{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 18: Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) merupakan cabang Machine Learning yang berfokus pada\n",
        "bagaimana sebuah **agen** dapat belajar mengambil keputusan optimal\n",
        "melalui interaksi langsung dengan sebuah **lingkungan (environment)**.\n",
        "\n",
        "Berbeda dengan supervised learning yang bergantung pada data berlabel,\n",
        "pada Reinforcement Learning agen belajar melalui proses *trial and error*.\n",
        "Agen melakukan suatu **aksi (action)**, kemudian menerima **umpan balik (reward)**,\n",
        "yang digunakan sebagai sinyal untuk memperbaiki perilaku di masa depan.\n",
        "\n",
        "Tujuan utama Reinforcement Learning adalah mempelajari sebuah **kebijakan (policy)**\n",
        "yang mampu memaksimalkan **total reward kumulatif** dalam jangka panjang,\n",
        "bukan hanya keuntungan sesaat.\n"
      ],
      "metadata": {
        "id": "ZR9JdGXS2OCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Konsep Utama\n",
        "\n",
        "Beberapa komponen fundamental dalam Reinforcement Learning meliputi:\n",
        "\n",
        "1. **Agen dan Lingkungan**  \n",
        "   Agen adalah entitas pembelajar, sedangkan lingkungan adalah dunia\n",
        "   tempat agen berinteraksi dan bereaksi terhadap tindakan agen.\n",
        "\n",
        "2. **State, Action, dan Reward**  \n",
        "   - *State* merepresentasikan kondisi lingkungan saat ini  \n",
        "   - *Action* adalah keputusan yang diambil agen  \n",
        "   - *Reward* adalah umpan balik numerik yang menilai kualitas aksi\n",
        "\n",
        "3. **Policy (Kebijakan)**  \n",
        "   Policy adalah strategi yang menentukan aksi apa yang harus diambil\n",
        "   oleh agen pada setiap state.\n",
        "\n",
        "4. **OpenAI Gym**  \n",
        "   Gym merupakan toolkit standar yang menyediakan berbagai environment\n",
        "   simulasi untuk mengembangkan dan mengevaluasi algoritma RL.\n"
      ],
      "metadata": {
        "id": "zPnypuWB2Rmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Menyiapkan Lingkungan dengan OpenAI Gym\n",
        "\n",
        "Sebagai contoh konkret, kita menggunakan environment klasik **CartPole**.\n",
        "Tugas agen adalah menjaga sebuah tiang agar tetap berdiri tegak\n",
        "dengan menggerakkan kereta ke kiri atau ke kanan.\n",
        "\n",
        "Meskipun terlihat sederhana, CartPole merupakan benchmark penting\n",
        "karena mencakup konsep keseimbangan, pengambilan keputusan cepat,\n",
        "dan reward yang tertunda.\n"
      ],
      "metadata": {
        "id": "de7NuAgr2T5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Membuat environment CartPole\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Reset environment untuk mendapatkan state awal\n",
        "obs = env.reset()\n",
        "print(\"State awal:\", obs)\n",
        "\n",
        "# Contoh aksi acak\n",
        "action = 1  # 0 = kiri, 1 = kanan\n",
        "obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(\"State baru:\", obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Episode selesai:\", done)\n"
      ],
      "metadata": {
        "id": "myZGRL7D2VHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Policies\n",
        "\n",
        "Alih-alih menggunakan aturan statis (hard-coded),\n",
        "kita dapat memanfaatkan **Neural Network** sebagai policy.\n",
        "Model ini menerima state lingkungan sebagai input\n",
        "dan menghasilkan probabilitas untuk setiap aksi.\n",
        "\n",
        "Pendekatan ini memungkinkan agen untuk belajar\n",
        "strategi yang jauh lebih fleksibel dan adaptif,\n",
        "terutama pada lingkungan yang kompleks dan berdimensi tinggi.\n"
      ],
      "metadata": {
        "id": "eLnmxFup2WGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "n_inputs = 4  # Jumlah fitur state pada CartPole\n",
        "\n",
        "policy_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(5, activation=\"relu\", input_shape=[n_inputs]),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")  # Probabilitas aksi ke kanan\n",
        "])\n",
        "\n",
        "print(\"Neural Network Policy berhasil dibuat.\")\n"
      ],
      "metadata": {
        "id": "nDiTuyWw2XFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Algoritma Policy Gradients (REINFORCE)\n",
        "\n",
        "Pendekatan **Policy Gradients** secara langsung mengoptimalkan policy,\n",
        "bukan nilai Q atau fungsi nilai lainnya.\n",
        "\n",
        "Algoritma REINFORCE bekerja dengan cara:\n",
        "1. Menjalankan agen selama satu atau beberapa episode penuh\n",
        "2. Menghitung total reward terdiskon untuk setiap langkah\n",
        "3. Memperkuat aksi yang menghasilkan reward tinggi\n",
        "   dan melemahkan aksi yang menghasilkan reward rendah\n",
        "\n",
        "Pendekatan ini sangat cocok untuk:\n",
        "- Action space kontinu\n",
        "- Masalah dengan kebijakan stokastik\n"
      ],
      "metadata": {
        "id": "Ii3sTXio2YCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [\n",
        "        discount_rewards(rewards, discount_factor)\n",
        "        for rewards in all_rewards\n",
        "    ]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    mean = flat_rewards.mean()\n",
        "    std = flat_rewards.std()\n",
        "    return [(rewards - mean) / std for rewards in all_discounted_rewards]\n",
        "\n",
        "print(\"Fungsi reward processing siap digunakan.\")\n"
      ],
      "metadata": {
        "id": "L3ULYDtd2ZDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Q-Learning dan Markov Decision Process (MDP)\n",
        "\n",
        "Sebagian besar masalah Reinforcement Learning dapat dimodelkan\n",
        "sebagai **Markov Decision Process (MDP)**,\n",
        "di mana state masa depan hanya bergantung pada state saat ini dan aksi yang diambil.\n",
        "\n",
        "Q-Learning merupakan algoritma berbasis nilai (*value-based*)\n",
        "yang bertujuan mempelajari fungsi Q(s, a),\n",
        "yaitu estimasi kualitas suatu aksi pada state tertentu.\n"
      ],
      "metadata": {
        "id": "9oje7bbO2aTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persamaan Bellman\n",
        "\n",
        "Q(s, a) ← (1 − α) Q(s, a) + α [ r + γ max Q(s′, a′) ]\n",
        "\n",
        "Di mana:\n",
        "- α : learning rate\n",
        "- γ : discount factor\n"
      ],
      "metadata": {
        "id": "1Fd0AVed2bUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Deep Q-Learning (DQN)\n",
        "\n",
        "Ketika ruang state menjadi sangat besar atau kontinu,\n",
        "pendekatan Q-table menjadi tidak praktis.\n",
        "Solusinya adalah menggunakan **Deep Neural Network**\n",
        "untuk mengaproksimasi fungsi Q.\n",
        "\n",
        "Pendekatan ini dikenal sebagai **Deep Q-Network (DQN)**,\n",
        "yang menjadi tonggak penting dalam kesuksesan RL modern,\n",
        "terutama pada game Atari.\n"
      ],
      "metadata": {
        "id": "mzDVSiRe2c49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0.1):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(2)\n",
        "    else:\n",
        "        Q_values = policy_model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(Q_values[0])\n",
        "\n",
        "print(\"Komponen inti Deep Q-Learning siap.\")\n"
      ],
      "metadata": {
        "id": "QgPe8Cb22eEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kesimpulan\n",
        "1. Reinforcement Learning berfokus pada pengambilan keputusan jangka panjang\n",
        "   dengan reward yang seringkali tertunda.\n",
        "2. Policy Gradient cocok untuk masalah kompleks dan aksi kontinu.\n",
        "3. Q-Learning dan DQN sangat efektif untuk lingkungan diskrit,\n",
        "   namun membutuhkan teknik stabilisasi tambahan.\n",
        "4. Untuk implementasi yang lebih matang,\n",
        "   library seperti **TF-Agents** atau **Stable-Baselines**\n",
        "   sangat direkomendasikan.\n"
      ],
      "metadata": {
        "id": "39Bod8yo2fKT"
      }
    }
  ]
}