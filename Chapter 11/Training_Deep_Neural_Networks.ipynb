{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 11: Training Deep Neural Networks\n",
        "\n",
        "Melatih **Deep Neural Networks (DNN)** merupakan tantangan tersendiri dibandingkan dengan neural network dangkal. Semakin dalam sebuah jaringan, semakin besar pula kompleksitas optimisasi yang harus dihadapi selama proses training.\n",
        "\n",
        "Deep Neural Networks mampu mempelajari representasi yang sangat kompleks, namun hal ini datang dengan sejumlah permasalahan serius yang dapat menghambat konvergensi, memperlambat pelatihan, atau bahkan menyebabkan model gagal belajar sama sekali.\n",
        "\n",
        "Pada chapter ini, kita akan membahas berbagai **masalah utama dalam pelatihan DNN** serta **teknik-teknik modern** yang dikembangkan untuk mengatasinya, berdasarkan praktik terbaik dalam deep learning.\n"
      ],
      "metadata": {
        "id": "JJLVSV36bQgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masalah Utama dalam Pelatihan Deep Neural Networks\n",
        "\n",
        "Beberapa permasalahan utama yang sering muncul ketika melatih DNN antara lain:\n",
        "\n",
        "1. **Vanishing dan Exploding Gradients**  \n",
        "   Gradien menjadi terlalu kecil atau terlalu besar saat backpropagation.\n",
        "\n",
        "2. **Fungsi Aktivasi yang Tidak Tepat**  \n",
        "   Fungsi aktivasi tertentu dapat menyebabkan saturasi dan memperlambat training.\n",
        "\n",
        "3. **Distribusi Input yang Tidak Stabil**  \n",
        "   Perubahan distribusi aktivasi antar layer memperlambat konvergensi.\n",
        "\n",
        "4. **Proses Training yang Lambat**  \n",
        "   DNN membutuhkan optimisasi yang efisien agar dapat dilatih dalam waktu yang masuk akal.\n",
        "\n",
        "5. **Overfitting**  \n",
        "   Model dengan jutaan parameter sangat mudah menghafal data latih.\n",
        "\n",
        "Chapter ini membahas solusi sistematis untuk setiap permasalahan tersebut.\n"
      ],
      "metadata": {
        "id": "ueAr-QsZbS22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mengatasi Vanishing & Exploding Gradients\n",
        "\n",
        "Masalah **vanishing gradient** terjadi ketika gradien menjadi sangat kecil saat disebarkan ke layer awal, sehingga bobot hampir tidak diperbarui.\n",
        "\n",
        "Sebaliknya, **exploding gradient** terjadi ketika gradien menjadi sangat besar dan menyebabkan pembaruan bobot yang tidak stabil.\n",
        "\n",
        "Kedua masalah ini sering muncul pada DNN karena:\n",
        "- banyaknya layer,\n",
        "- penggunaan fungsi aktivasi yang tidak tepat,\n",
        "- inisialisasi bobot yang buruk.\n"
      ],
      "metadata": {
        "id": "y3rltPXHbUQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan Leaky ReLU\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\n",
        "])\n",
        "\n",
        "# Menggunakan SELU untuk Self-Normalization\n",
        "layer_selu = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ACQgUQsfcEdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inisialisasi Bobot yang Tepat (He Initialization)\n",
        "\n",
        "Salah satu solusi paling efektif untuk mengatasi masalah gradien adalah menggunakan **inisialisasi bobot yang sesuai dengan fungsi aktivasi**.\n",
        "\n",
        "**He Initialization** dirancang khusus untuk fungsi aktivasi ReLU dan variannya.\n",
        "\n",
        "Prinsip dasarnya adalah menjaga varians aktivasi tetap stabil di seluruh layer selama forward dan backward pass.\n"
      ],
      "metadata": {
        "id": "L5-kEhQlbVXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "layer = keras.layers.Dense(\n",
        "    10,\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=\"he_normal\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "TvQgSjXcbbBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan He Initialization:\n",
        "- gradien tidak cepat menghilang,\n",
        "- training menjadi lebih stabil,\n",
        "- konvergensi lebih cepat.\n",
        "\n",
        "Teknik ini hampir menjadi standar de-facto untuk DNN berbasis ReLU.\n"
      ],
      "metadata": {
        "id": "dIEjJN5LbcwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fungsi Aktivasi Non-Saturasi\n",
        "\n",
        "Fungsi aktivasi seperti **sigmoid** dan **tanh** cenderung mengalami saturasi pada nilai input ekstrem, yang menyebabkan gradien mendekati nol.\n",
        "\n",
        "Untuk mengatasi masalah ini, digunakan fungsi aktivasi **non-saturasi** yang tetap memiliki gradien signifikan pada sebagian besar domain input.\n"
      ],
      "metadata": {
        "id": "V4PQHv-Gbe7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternatif Fungsi Aktivasi Modern\n",
        "\n",
        "Beberapa fungsi aktivasi non-saturasi yang populer meliputi:\n",
        "\n",
        "- **Leaky ReLU**  \n",
        "  Mengizinkan gradien kecil pada nilai negatif untuk mencegah neuron mati.\n",
        "\n",
        "- **ELU (Exponential Linear Unit)**  \n",
        "  Memberikan transisi halus di sekitar nol dan mempercepat konvergensi.\n",
        "\n",
        "- **SELU (Scaled ELU)**  \n",
        "  Dirancang untuk menciptakan *self-normalizing networks* jika dikombinasikan dengan inisialisasi yang tepat.\n"
      ],
      "metadata": {
        "id": "HHliCG-tbgBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leaky ReLU\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\n",
        "])\n",
        "\n",
        "# SELU dengan self-normalization\n",
        "layer_selu = keras.layers.Dense(\n",
        "    10,\n",
        "    activation=\"selu\",\n",
        "    kernel_initializer=\"lecun_normal\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "-xqZx_I8bhDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penggunaan fungsi aktivasi yang tepat:\n",
        "- meningkatkan aliran gradien,\n",
        "- mempercepat pelatihan,\n",
        "- mengurangi risiko kegagalan konvergensi.\n"
      ],
      "metadata": {
        "id": "FrtecdLLbiHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Batch Normalization\n",
        "\n",
        "**Batch Normalization (BN)** adalah teknik yang menormalkan input pada setiap layer selama training.\n",
        "\n",
        "BN mengurangi masalah yang disebut *internal covariate shift*, yaitu perubahan distribusi aktivasi antar layer selama training.\n"
      ],
      "metadata": {
        "id": "rrLX2CzqbjML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manfaat utama Batch Normalization:\n",
        "- mempercepat konvergensi,\n",
        "- memungkinkan learning rate lebih besar,\n",
        "- mengurangi sensitivitas terhadap inisialisasi bobot,\n",
        "- bertindak sebagai regularisasi ringan.\n"
      ],
      "metadata": {
        "id": "iPfU8WcybkTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "MCPqqyViblJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization sangat dianjurkan untuk jaringan yang dalam dan kompleks.\n"
      ],
      "metadata": {
        "id": "gmOG3XD6bmDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transfer Learning (Reusing Layers)\n",
        "\n",
        "Transfer Learning memungkinkan kita memanfaatkan pengetahuan dari model yang sudah dilatih sebelumnya.\n",
        "\n",
        "Alih-alih melatih model dari nol, kita:\n",
        "- menggunakan layer awal dari model lama,\n",
        "- hanya melatih layer akhir untuk tugas baru.\n"
      ],
      "metadata": {
        "id": "vE9d7I2mbm74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pendekatan ini sangat efektif ketika:\n",
        "- data berlabel terbatas,\n",
        "- tugas baru masih mirip dengan tugas lama,\n",
        "- training dari awal terlalu mahal secara komputasi.\n"
      ],
      "metadata": {
        "id": "3WXXS2fHboT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh konsep transfer learning\n",
        "\n",
        "# model_A = keras.models.load_model(\"model_A.h5\")\n",
        "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Membekukan layer lama\n",
        "# for layer in model_B_on_A.layers[:-1]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# model_B_on_A.compile(\n",
        "#     loss=\"binary_crossentropy\",\n",
        "#     optimizer=\"sgd\",\n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n"
      ],
      "metadata": {
        "id": "tir-XijpbpRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Misalkan kita memuat model A\n",
        "# model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "# Membuat model B berdasarkan model A tanpa lapisan output terakhir\n",
        "# model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "# model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Membekukan (Freezing) lapisan yang digunakan kembali\n",
        "# for layer in model_B_on_A.layers[:-1]:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# Jangan lupa untuk compile setelah mengubah status trainable\n",
        "# model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        ""
      ],
      "metadata": {
        "id": "VsAg4fY2cL6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning secara signifikan mengurangi waktu training dan meningkatkan performa awal model.\n"
      ],
      "metadata": {
        "id": "gLb3O0mSbqN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Optimizer Cepat\n",
        "\n",
        "Optimizer menentukan bagaimana bobot diperbarui berdasarkan gradien.\n",
        "\n",
        "SGD standar sering kali terlalu lambat untuk DNN besar.\n"
      ],
      "metadata": {
        "id": "2WBxTuG7brdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer modern yang populer meliputi:\n",
        "- **Momentum**\n",
        "- **Nesterov Accelerated Gradient (NAG)**\n",
        "- **RMSProp**\n",
        "- **Adam**\n"
      ],
      "metadata": {
        "id": "CBN2Z7f-bssC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999\n",
        ")\n"
      ],
      "metadata": {
        "id": "nxqDMoJlbvLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam menggabungkan keunggulan Momentum dan RMSProp, sehingga sering menjadi pilihan default dalam deep learning.\n"
      ],
      "metadata": {
        "id": "phjR3jwZbwC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Regularisasi untuk Mencegah Overfitting\n",
        "\n",
        "DNN dengan kapasitas besar sangat rentan terhadap overfitting.\n",
        "\n",
        "Salah satu teknik regularisasi paling efektif adalah **Dropout**.\n"
      ],
      "metadata": {
        "id": "fuyv6hGIbw_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout bekerja dengan:\n",
        "- menonaktifkan neuron secara acak selama training,\n",
        "- memaksa jaringan untuk belajar representasi yang lebih robust,\n",
        "- mengurangi ketergantungan pada neuron tertentu.\n"
      ],
      "metadata": {
        "id": "8HwBET6VbyDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "O4LOLVZfbzRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout sering dikombinasikan dengan:\n",
        "- Early Stopping,\n",
        "- Batch Normalization,\n",
        "- Data Augmentation.\n"
      ],
      "metadata": {
        "id": "B9H7FSbKb23o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kesimpulan (Chapter 11)\n",
        "\n",
        "Berdasarkan praktik terbaik dalam pelatihan Deep Neural Networks, konfigurasi umum yang direkomendasikan adalah:\n",
        "\n",
        "- Inisialisasi bobot: **He Initialization**\n",
        "- Fungsi aktivasi: **ELU atau ReLU**\n",
        "- Normalisasi: **Batch Normalization**\n",
        "- Regularisasi: **Dropout + Early Stopping**\n",
        "- Optimizer: **Adam atau Nadam**\n",
        "- Learning rate: gunakan scheduler jika memungkinkan\n",
        "\n",
        "Penerapan teknik-teknik ini secara konsisten dapat meningkatkan stabilitas, kecepatan konvergensi, dan performa akhir model deep learning.\n"
      ],
      "metadata": {
        "id": "mU1pv3TQb3_r"
      }
    }
  ]
}