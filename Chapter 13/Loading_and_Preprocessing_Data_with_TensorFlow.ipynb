{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "Pada praktik Machine Learning modern, sering kali kita berhadapan dengan dataset berukuran sangat besar, bahkan melebihi kapasitas memori (RAM). Oleh karena itu, diperlukan mekanisme yang efisien untuk membaca, memproses, dan mengalirkan data ke model tanpa menyebabkan bottleneck pada proses training.\n",
        "\n",
        "Bab ini membahas **TensorFlow Data API (`tf.data`)**, sebuah framework powerful yang memungkinkan kita membangun *input pipeline* yang efisien, scalable, dan teroptimasi. Dengan pipeline yang baik, GPU atau TPU dapat bekerja secara maksimal tanpa harus menunggu data disiapkan oleh CPU.\n",
        "\n",
        "## Daftar Isi:\n",
        "1. **The Data API**: Konsep dasar dan objek `Dataset`.\n",
        "2. **Transformasi Data**: Proses chaining, shuffling, batching, dan prefetching.\n",
        "3. **Membaca CSV Skala Besar**: Strategi membaca banyak file secara paralel.\n",
        "4. **Format TFRecord**: Format biner efisien untuk data skala besar.\n",
        "5. **Keras Preprocessing Layers**: Normalisasi dan encoding langsung di dalam model.\n"
      ],
      "metadata": {
        "id": "91Hanx17hm6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dasar-Dasar Data API\n",
        "\n",
        "TensorFlow Data API menyediakan abstraksi tingkat tinggi berupa objek `Dataset`, yang memungkinkan kita membaca data dari berbagai sumber seperti array di memori, file teks, CSV, hingga TFRecord.\n",
        "\n",
        "Pendekatan ini membuat proses input data:\n",
        "- Lebih konsisten\n",
        "- Mudah dioptimalkan\n",
        "- Cocok untuk pipeline training berskala besar\n",
        "\n",
        "Dataset bersifat *lazy*, artinya data hanya dibaca ketika dibutuhkan, sehingga sangat efisien dalam penggunaan memori.\n"
      ],
      "metadata": {
        "id": "3JCayMnghoze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Membuat dataset dari Python list\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "\n",
        "# Menampilkan isi dataset\n",
        "for item in dataset:\n",
        "    print(item.numpy())\n"
      ],
      "metadata": {
        "id": "hRNTcFAQhp07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Rantai Transformasi (Chaining Transformations)\n",
        "\n",
        "Salah satu keunggulan utama Data API adalah kemampuannya melakukan transformasi data secara berurutan (*chaining*). Setiap operasi akan menghasilkan objek `Dataset` baru yang dapat diproses lebih lanjut.\n",
        "\n",
        "Transformasi ini memungkinkan kita:\n",
        "- Mengulang dataset beberapa kali\n",
        "- Membagi data ke dalam batch\n",
        "- Mengontrol alur data secara fleksibel\n",
        "\n",
        "Pendekatan ini sangat berguna dalam skenario training berulang (epoch).\n"
      ],
      "metadata": {
        "id": "PRtXQBBIhqlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengulang dataset 3 kali dan mengelompokkan dalam batch berisi 7\n",
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "\n",
        "for item in dataset:\n",
        "    print(\"Batch:\", item.numpy())\n"
      ],
      "metadata": {
        "id": "tINwyLathtdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map, Filter, dan Shuffle\n",
        "\n",
        "Transformasi lanjutan yang paling sering digunakan meliputi:\n",
        "\n",
        "- **`map()`**: Mengaplikasikan fungsi ke setiap elemen dataset, misalnya scaling atau normalisasi.\n",
        "- **`filter()`**: Menyaring data berdasarkan kondisi tertentu.\n",
        "- **`shuffle()`**: Mengacak urutan data untuk menghindari bias urutan saat training.\n",
        "\n",
        "Kombinasi ketiga operasi ini sangat penting untuk memastikan model belajar secara stabil dan tidak menghafal urutan data.\n"
      ],
      "metadata": {
        "id": "6dNnnInrhurl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "\n",
        "# Map: kalikan dua, Filter: ambil yang > 10, Shuffle: acak dengan buffer\n",
        "dataset = dataset.map(lambda x: x * 2) \\\n",
        "                 .filter(lambda x: x > 10) \\\n",
        "                 .shuffle(buffer_size=5, seed=42) \\\n",
        "                 .batch(3)\n",
        "\n",
        "for item in dataset:\n",
        "    print(\"Filtered & Shuffled Batch:\", item.numpy())\n"
      ],
      "metadata": {
        "id": "nagTC7wwhvmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Optimasi Pipeline dengan Prefetching\n",
        "\n",
        "Salah satu penyebab utama training lambat adalah ketidakseimbangan antara CPU dan GPU. Saat GPU sedang idle menunggu data, efisiensi sistem menurun drastis.\n",
        "\n",
        "**Prefetching** mengatasi masalah ini dengan cara menyiapkan batch berikutnya saat batch saat ini sedang diproses oleh model. Dengan demikian, CPU dan GPU dapat bekerja secara paralel.\n"
      ],
      "metadata": {
        "id": "b9NTC1v5hxdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gunakan tf.data.AUTOTUNE agar TensorFlow menentukan jumlah prefetch optimal\n",
        "dataset = tf.data.Dataset.range(10).batch(3).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "sSZwSWjahyi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Membaca Banyak File CSV secara Paralel\n",
        "\n",
        "Pada dataset besar, data sering dibagi ke dalam banyak file CSV. Membaca file secara sekuensial akan memperlambat training.\n",
        "\n",
        "Dengan menggunakan `interleave`, TensorFlow dapat membaca baris dari beberapa file sekaligus, sehingga throughput data meningkat secara signifikan.\n"
      ],
      "metadata": {
        "id": "H473QCoShzXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_line(line):\n",
        "    # Contoh parsing CSV: 8 fitur float dan 1 label float\n",
        "    defaults = [0.] * 8 + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defaults)\n",
        "    return tf.stack(fields[:-1]), fields[-1]\n",
        "\n",
        "# file_paths = [\"file1.csv\", \"file2.csv\", ...]\n",
        "# dataset = tf.data.Dataset.list_files(file_paths)\n",
        "# dataset = dataset.interleave(\n",
        "#     lambda path: tf.data.TextLineDataset(path).skip(1),\n",
        "#     cycle_length=5)\n",
        "# dataset = dataset.map(parse_line, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "FAMhiFRBh1IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Format Data TFRecord\n",
        "\n",
        "TFRecord merupakan format penyimpanan biner bawaan TensorFlow yang dirancang untuk efisiensi tinggi. Format ini sangat cocok digunakan ketika dataset berukuran sangat besar atau digunakan dalam sistem terdistribusi.\n",
        "\n",
        "Keunggulan TFRecord:\n",
        "- Lebih cepat dibaca dibanding CSV\n",
        "- Ukuran file lebih kecil\n",
        "- Terintegrasi langsung dengan ekosistem TensorFlow\n"
      ],
      "metadata": {
        "id": "bVZz1LLAh2c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menulis TFRecord\n",
        "with tf.io.TFRecordWriter(\"data_latihan.tfrecord\") as f:\n",
        "    f.write(b\"Data biner pertama\")\n",
        "    f.write(b\"Data biner kedua\")\n",
        "\n",
        "# Membaca TFRecord\n",
        "dataset = tf.data.TFRecordDataset([\"data_latihan.tfrecord\"])\n",
        "for item in dataset:\n",
        "    print(item.numpy())\n"
      ],
      "metadata": {
        "id": "C_d2PQ4kh3aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Keras Preprocessing Layers\n",
        "\n",
        "TensorFlow menyediakan preprocessing layer yang dapat dimasukkan langsung ke dalam model. Pendekatan ini membuat preprocessing:\n",
        "- Konsisten antara training dan inference\n",
        "- Mudah dipindahkan ke production\n",
        "- Lebih aman dari data leakage\n",
        "\n",
        "Layer preprocessing dipelajari menggunakan metode `.adapt()`.\n"
      ],
      "metadata": {
        "id": "59KC0FKRh4p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Layer Normalisasi\n",
        "norm_layer = layers.Normalization()\n",
        "sample_data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=\"float32\")\n",
        "norm_layer.adapt(sample_data)\n",
        "\n",
        "# Layer StringLookup (untuk kategori)\n",
        "lookup = layers.StringLookup()\n",
        "lookup.adapt([\"Kucing\", \"Anjing\", \"Burung\"])\n",
        "print(\"Encoding 'Anjing':\", lookup(tf.constant([\"Anjing\"])).numpy())\n",
        "\n",
        "# Mengintegrasikan ke dalam model\n",
        "model = tf.keras.models.Sequential([\n",
        "    norm_layer,\n",
        "    layers.Dense(10, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n"
      ],
      "metadata": {
        "id": "Tr4l6LhTh5j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rangkuman Praktis\n",
        "\n",
        "Bab ini menekankan pentingnya input pipeline yang efisien dalam Machine Learning skala besar. Beberapa poin penting yang perlu diingat:\n",
        "\n",
        "1. Gunakan **prefetching** untuk memaksimalkan pemanfaatan hardware.\n",
        "2. Paralelkan proses `map()` untuk mempercepat preprocessing.\n",
        "3. Gunakan **TFRecord** untuk performa I/O terbaik.\n",
        "4. Pastikan preprocessing layer dipelajari menggunakan `.adapt()` sebelum training.\n",
        "\n",
        "Dengan pipeline data yang baik, proses training menjadi lebih cepat, stabil, dan siap untuk deployment skala produksi.\n"
      ],
      "metadata": {
        "id": "U6xzHBzqh63V"
      }
    }
  ]
}